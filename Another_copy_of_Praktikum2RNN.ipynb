{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " ! pip install -q kaggle"
      ],
      "metadata": {
        "id": "SsJ6VwW9IuGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "A8FrwjC8IxSW",
        "outputId": "27492041-9d79-4c50-cafc-58ac6a2d5cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74401b39-41f4-48e2-84c3-286242b224cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74401b39-41f4-48e2-84c3-286242b224cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ronggurmwp\",\"key\":\"659addeda6fe20f4cdbb154d65d80c1b\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "R09vTlLCI-Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "FP2E4CPBJAVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MWXQHrqIpPD",
        "outputId": "e53aa6ac-8546-4b65-8042-23905abad1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading praktikum-2-rnn-if4074-2023.zip to /content\n",
            "\r  0% 0.00/5.21k [00:00<?, ?B/s]\n",
            "\r100% 5.21k/5.21k [00:00<00:00, 16.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c praktikum-2-rnn-if4074-2023"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip praktikum-2-rnn-if4074-2023.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WErOc4lZIq9y",
        "outputId": "9c282565-d720-4871-8bee-a09a39c598c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  praktikum-2-rnn-if4074-2023.zip\n",
            "  inflating: test_LTC.csv            \n",
            "  inflating: train_LTC.csv           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "rZzOtWlxJNmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# data di preprocess menggunakan excel\n",
        "df_train = pd.read_csv('train_LTC.csv', delimiter=',')\n",
        "df_test = pd.read_csv('test_LTC.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "xjh3JN9wJdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8gT8b2wGfae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "ryDfvUhkJ7Bp",
        "outputId": "76f85c60-1692-4e70-bc2c-f2a189c133b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date   Open   High    Low  Close\n",
              "0   2018-01-12  31.88  34.99  31.00  34.22\n",
              "1   2018-02-12  34.22  34.82  32.81  33.20\n",
              "2   2018-03-12  33.20  33.20  29.62  30.53\n",
              "3   2018-04-12  30.53  31.88  29.66  30.77\n",
              "4   2018-05-12  30.77  31.06  28.67  28.70\n",
              "5   2018-06-12  28.70  30.09  25.80  26.22\n",
              "6   2018-07-12  26.22  26.55  22.17  24.96\n",
              "7   2018-08-12  24.96  25.74  22.94  24.00\n",
              "8   2018-09-12  24.00  26.60  24.00  25.23\n",
              "9   2018-10-12  25.23  25.66  23.62  24.12\n",
              "10  2018-11-12  24.12  24.76  22.69  23.23\n",
              "11  2018-11-13  50.50  50.50  48.35  49.20\n",
              "12  2018-11-14  49.20  49.90  40.24  42.72\n",
              "13  2018-11-15  42.72  43.73  39.59  43.20\n",
              "14  2018-11-16  43.20  43.61  41.00  41.72\n",
              "15  2018-11-17  41.72  42.34  41.06  41.65\n",
              "16  2018-11-18  41.65  42.88  41.50  41.85\n",
              "17  2018-11-19  41.85  41.85  35.00  35.91\n",
              "18  2018-11-20  35.91  37.55  30.22  32.03\n",
              "19  2018-11-21  32.03  34.34  31.10  34.33"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2722453a-3fbe-48c7-8859-9e3a9ec4848a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-12</td>\n",
              "      <td>31.88</td>\n",
              "      <td>34.99</td>\n",
              "      <td>31.00</td>\n",
              "      <td>34.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-02-12</td>\n",
              "      <td>34.22</td>\n",
              "      <td>34.82</td>\n",
              "      <td>32.81</td>\n",
              "      <td>33.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-03-12</td>\n",
              "      <td>33.20</td>\n",
              "      <td>33.20</td>\n",
              "      <td>29.62</td>\n",
              "      <td>30.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-12</td>\n",
              "      <td>30.53</td>\n",
              "      <td>31.88</td>\n",
              "      <td>29.66</td>\n",
              "      <td>30.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-05-12</td>\n",
              "      <td>30.77</td>\n",
              "      <td>31.06</td>\n",
              "      <td>28.67</td>\n",
              "      <td>28.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2018-06-12</td>\n",
              "      <td>28.70</td>\n",
              "      <td>30.09</td>\n",
              "      <td>25.80</td>\n",
              "      <td>26.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>26.22</td>\n",
              "      <td>26.55</td>\n",
              "      <td>22.17</td>\n",
              "      <td>24.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2018-08-12</td>\n",
              "      <td>24.96</td>\n",
              "      <td>25.74</td>\n",
              "      <td>22.94</td>\n",
              "      <td>24.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018-09-12</td>\n",
              "      <td>24.00</td>\n",
              "      <td>26.60</td>\n",
              "      <td>24.00</td>\n",
              "      <td>25.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2018-10-12</td>\n",
              "      <td>25.23</td>\n",
              "      <td>25.66</td>\n",
              "      <td>23.62</td>\n",
              "      <td>24.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2018-11-12</td>\n",
              "      <td>24.12</td>\n",
              "      <td>24.76</td>\n",
              "      <td>22.69</td>\n",
              "      <td>23.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2018-11-13</td>\n",
              "      <td>50.50</td>\n",
              "      <td>50.50</td>\n",
              "      <td>48.35</td>\n",
              "      <td>49.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2018-11-14</td>\n",
              "      <td>49.20</td>\n",
              "      <td>49.90</td>\n",
              "      <td>40.24</td>\n",
              "      <td>42.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2018-11-15</td>\n",
              "      <td>42.72</td>\n",
              "      <td>43.73</td>\n",
              "      <td>39.59</td>\n",
              "      <td>43.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2018-11-16</td>\n",
              "      <td>43.20</td>\n",
              "      <td>43.61</td>\n",
              "      <td>41.00</td>\n",
              "      <td>41.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2018-11-17</td>\n",
              "      <td>41.72</td>\n",
              "      <td>42.34</td>\n",
              "      <td>41.06</td>\n",
              "      <td>41.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2018-11-18</td>\n",
              "      <td>41.65</td>\n",
              "      <td>42.88</td>\n",
              "      <td>41.50</td>\n",
              "      <td>41.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2018-11-19</td>\n",
              "      <td>41.85</td>\n",
              "      <td>41.85</td>\n",
              "      <td>35.00</td>\n",
              "      <td>35.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2018-11-20</td>\n",
              "      <td>35.91</td>\n",
              "      <td>37.55</td>\n",
              "      <td>30.22</td>\n",
              "      <td>32.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2018-11-21</td>\n",
              "      <td>32.03</td>\n",
              "      <td>34.34</td>\n",
              "      <td>31.10</td>\n",
              "      <td>34.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2722453a-3fbe-48c7-8859-9e3a9ec4848a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2722453a-3fbe-48c7-8859-9e3a9ec4848a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2722453a-3fbe-48c7-8859-9e3a9ec4848a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7b767d5-2b3c-46a5-b0b8-963350a54944\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7b767d5-2b3c-46a5-b0b8-963350a54944')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7b767d5-2b3c-46a5-b0b8-963350a54944 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bnWA3wNZMKJ9",
        "outputId": "11a38a3a-0e8b-4a1b-9690-ddb44da3c94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date\n",
              "0  2019-11-27\n",
              "1  2019-11-28\n",
              "2  2019-11-29\n",
              "3  2019-11-30\n",
              "4  2019-12-01"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88d67631-7786-430e-bd6a-625233ed3dae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-11-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-11-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-11-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-11-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-12-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88d67631-7786-430e-bd6a-625233ed3dae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88d67631-7786-430e-bd6a-625233ed3dae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88d67631-7786-430e-bd6a-625233ed3dae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-818ef5af-ddd2-472a-8209-da5327e08791\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-818ef5af-ddd2-472a-8209-da5327e08791')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-818ef5af-ddd2-472a-8209-da5327e08791 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(15,5))\n",
        "plt.plot(df_train['Close'])\n",
        "plt.ylabel('Close price')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "dXXa-jcqGgdo",
        "outputId": "33b74cc7-1fbe-46e5-c25c-140cf6f442aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMPXEBTwKTAF",
        "outputId": "11a08169-0104-479c-baf0-147ff78ff8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "3mFCh8JRGqYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPz88UpXMQ0l",
        "outputId": "e628460d-eb25-4fe6-f8d6-4f2f806e3e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_5e8mRpGtnT",
        "outputId": "f4f26033-0d65-4f22-b26e-e37aee3ddf9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 168 entries, 0 to 167\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Date    168 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Bidirectional\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ZCgNMrkYJ8gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_train_scaled = scaler.fit_transform(df_train[['Open', 'Close', 'High', 'Low']])\n",
        "# scaled_data = scaler.fit_transform(df_test[['Open', 'Close', 'High', 'Low']])"
      ],
      "metadata": {
        "id": "GdhGr6NLJ_vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train_scaled.head()"
      ],
      "metadata": {
        "id": "KGnJr-J6NspJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    print(len(data) - seq_length)\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "id": "Oz1JR_3KKBrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 3  # You can adjust the sequence length\n",
        "X_train, y_train = create_sequences(df_train_scaled, seq_length)\n",
        "# X_test, y_test = create_sequences(df_train_scaled, seq_length)\n",
        "\n",
        "# seq_length_variations = [3, 5 , 7, 10, 15, 25, 40]\n",
        "seq_length_variations = [1,2,3, 5, 10, 15, 25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gDG7RJELsyb",
        "outputId": "ec2c9c44-d838-493f-a816-c32cefa68888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
        "def custom_accuracy(y_true, y_pred, admitted_range=0.1):\n",
        "    lower_bound = y_true - admitted_range\n",
        "    upper_bound = y_true + admitted_range\n",
        "\n",
        "    is_within_range = tf.math.logical_and(y_pred >= lower_bound, y_pred <= upper_bound)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_within_range, tf.float32))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ISkBcIPRtubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom callback to print the last epoch\n",
        "class PrintLastEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "      self.epochs = 100\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch == (self.epochs - 1):\n",
        "            print(logs)"
      ],
      "metadata": {
        "id": "exRuOTMtvvTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with 8 stacks\n",
        "models = []\n",
        "last_trains = []\n",
        "for i in range(len(seq_length_variations)):\n",
        "  seq_length = seq_length_variations[i]\n",
        "  X_train, y_train = create_sequences(df_train_scaled, seq_length)\n",
        "  model_lstm = Sequential()\n",
        "  model_lstm.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(seq_length, 4)))\n",
        "  model_lstm.add(Bidirectional(LSTM(10)))\n",
        "  model_lstm.add(Dense(4))  # Output layer has 4 units to predict all features\n",
        "\n",
        "  model_lstm.compile(optimizer='adam', loss=rmse, metrics=[custom_accuracy])\n",
        "  model_lstm.summary()\n",
        "  model_lstm.fit(X_train, y_train, epochs=100, batch_size=5, verbose = 0, callbacks=[PrintLastEpoch()])\n",
        "  models.append(model_lstm)\n",
        "  last_trains.append(X_train[len(X_train) - 1:])\n"
      ],
      "metadata": {
        "id": "_VXMrgBKQXjR",
        "outputId": "4a6625a6-a617-429b-a960-3c94f85c1f4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "379\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 1, 20)             1200      \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 20)                2480      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "76/76 [==============================] - 12s 7ms/step - loss: 0.3182 - custom_accuracy: 0.2952\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1429 - custom_accuracy: 0.5007\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1202 - custom_accuracy: 0.6789\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1151 - custom_accuracy: 0.7421\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1127 - custom_accuracy: 0.7584\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1135 - custom_accuracy: 0.7510\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1141 - custom_accuracy: 0.7414\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1117 - custom_accuracy: 0.7526\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1132 - custom_accuracy: 0.7467\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1157 - custom_accuracy: 0.7410\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1127 - custom_accuracy: 0.7454\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1160 - custom_accuracy: 0.7382\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1120 - custom_accuracy: 0.7401\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1156 - custom_accuracy: 0.7556\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1106 - custom_accuracy: 0.7401\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7566\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7375\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1117 - custom_accuracy: 0.7408\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1136 - custom_accuracy: 0.7349\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1105 - custom_accuracy: 0.7446\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1141 - custom_accuracy: 0.7414\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1106 - custom_accuracy: 0.7355\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1105 - custom_accuracy: 0.7474\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1120 - custom_accuracy: 0.7408\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1127 - custom_accuracy: 0.7283\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1117 - custom_accuracy: 0.7257\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1131 - custom_accuracy: 0.7257\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1110 - custom_accuracy: 0.7421\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1126 - custom_accuracy: 0.7474\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7342\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1162 - custom_accuracy: 0.7324\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1141 - custom_accuracy: 0.7414\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1132 - custom_accuracy: 0.7395\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1118 - custom_accuracy: 0.7313\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1105 - custom_accuracy: 0.7520\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7322\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7493\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1122 - custom_accuracy: 0.7454\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1112 - custom_accuracy: 0.7433\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1130 - custom_accuracy: 0.7368\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1132 - custom_accuracy: 0.7289\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1134 - custom_accuracy: 0.7309\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1134 - custom_accuracy: 0.7327\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1082 - custom_accuracy: 0.7428\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1122 - custom_accuracy: 0.7507\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1113 - custom_accuracy: 0.7375\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1112 - custom_accuracy: 0.7336\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1147 - custom_accuracy: 0.7434\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1167 - custom_accuracy: 0.7303\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1105 - custom_accuracy: 0.7401\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1132 - custom_accuracy: 0.7382\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1118 - custom_accuracy: 0.7368\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1102 - custom_accuracy: 0.7329\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1127 - custom_accuracy: 0.7428\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1123 - custom_accuracy: 0.7395\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1111 - custom_accuracy: 0.7462\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1118 - custom_accuracy: 0.7301\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1135 - custom_accuracy: 0.7301\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1099 - custom_accuracy: 0.7436\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1117 - custom_accuracy: 0.7309\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1107 - custom_accuracy: 0.7303\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1106 - custom_accuracy: 0.7362\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1103 - custom_accuracy: 0.7442\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1115 - custom_accuracy: 0.7250\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1105 - custom_accuracy: 0.7454\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1113 - custom_accuracy: 0.7382\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1114 - custom_accuracy: 0.7368\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1081 - custom_accuracy: 0.7507\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1137 - custom_accuracy: 0.7434\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1109 - custom_accuracy: 0.7382\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1106 - custom_accuracy: 0.7401\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1140 - custom_accuracy: 0.7161\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1094 - custom_accuracy: 0.7342\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1118 - custom_accuracy: 0.7465\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1115 - custom_accuracy: 0.7434\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1134 - custom_accuracy: 0.7262\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1094 - custom_accuracy: 0.7362\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1128 - custom_accuracy: 0.7408\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1124 - custom_accuracy: 0.7322\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1091 - custom_accuracy: 0.7447\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1125 - custom_accuracy: 0.7253\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1123 - custom_accuracy: 0.7316\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1148 - custom_accuracy: 0.7408\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1126 - custom_accuracy: 0.7316\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1118 - custom_accuracy: 0.7362\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1086 - custom_accuracy: 0.7401\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1115 - custom_accuracy: 0.7410\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1118 - custom_accuracy: 0.7382\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1118 - custom_accuracy: 0.7493\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1127 - custom_accuracy: 0.7421\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1101 - custom_accuracy: 0.7474\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1106 - custom_accuracy: 0.7380\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1104 - custom_accuracy: 0.7382\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1103 - custom_accuracy: 0.7368\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1114 - custom_accuracy: 0.7495\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1096 - custom_accuracy: 0.7454\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1110 - custom_accuracy: 0.7428\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1111 - custom_accuracy: 0.7500\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1114 - custom_accuracy: 0.7283\n",
            "Epoch 100/100\n",
            "75/76 [============================>.] - ETA: 0s - loss: 0.1094 - custom_accuracy: 0.7400{'loss': 0.10993786156177521, 'custom_accuracy': 0.7384867668151855}\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1099 - custom_accuracy: 0.7385\n",
            "378\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_2 (Bidirecti  (None, 2, 20)             1200      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 20)                2480      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "76/76 [==============================] - 7s 6ms/step - loss: 0.2565 - custom_accuracy: 0.3686\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1356 - custom_accuracy: 0.6224\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1324 - custom_accuracy: 0.6708\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1301 - custom_accuracy: 0.6774\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1251 - custom_accuracy: 0.6750\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1268 - custom_accuracy: 0.6857\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1236 - custom_accuracy: 0.6908\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1246 - custom_accuracy: 0.6829\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1231 - custom_accuracy: 0.6855\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1220 - custom_accuracy: 0.6912\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1269 - custom_accuracy: 0.6737\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1197 - custom_accuracy: 0.6842\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1195 - custom_accuracy: 0.6757\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1176 - custom_accuracy: 0.6875\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1169 - custom_accuracy: 0.7011\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1165 - custom_accuracy: 0.6987\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1141 - custom_accuracy: 0.6906\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1178 - custom_accuracy: 0.6941\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1150 - custom_accuracy: 0.7015\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1115 - custom_accuracy: 0.7033\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1146 - custom_accuracy: 0.7114\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1151 - custom_accuracy: 0.7270\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1150 - custom_accuracy: 0.7167\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7118\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1137 - custom_accuracy: 0.7272\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1140 - custom_accuracy: 0.7263\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7232\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1122 - custom_accuracy: 0.7226\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1113 - custom_accuracy: 0.7311\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1122 - custom_accuracy: 0.7300\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1085 - custom_accuracy: 0.7338\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1131 - custom_accuracy: 0.7467\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7329\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1127 - custom_accuracy: 0.7259\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1109 - custom_accuracy: 0.7412\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1114 - custom_accuracy: 0.7395\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7507\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1088 - custom_accuracy: 0.7342\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1095 - custom_accuracy: 0.7414\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1091 - custom_accuracy: 0.7430\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1110 - custom_accuracy: 0.7487\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1131 - custom_accuracy: 0.7289\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1099 - custom_accuracy: 0.7439\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1127 - custom_accuracy: 0.7586\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1100 - custom_accuracy: 0.7463\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1091 - custom_accuracy: 0.7507\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1106 - custom_accuracy: 0.7355\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1081 - custom_accuracy: 0.7544\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1080 - custom_accuracy: 0.7360\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1111 - custom_accuracy: 0.7489\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1106 - custom_accuracy: 0.7553\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1092 - custom_accuracy: 0.7502\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1118 - custom_accuracy: 0.7493\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1066 - custom_accuracy: 0.7502\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1082 - custom_accuracy: 0.7533\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1121 - custom_accuracy: 0.7487\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1101 - custom_accuracy: 0.7535\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1099 - custom_accuracy: 0.7454\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1121 - custom_accuracy: 0.7450\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1053 - custom_accuracy: 0.7467\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1095 - custom_accuracy: 0.7355\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1083 - custom_accuracy: 0.7575\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1081 - custom_accuracy: 0.7509\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1085 - custom_accuracy: 0.7450\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1079 - custom_accuracy: 0.7476\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1074 - custom_accuracy: 0.7404\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1082 - custom_accuracy: 0.7553\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1116 - custom_accuracy: 0.7491\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1092 - custom_accuracy: 0.7520\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1091 - custom_accuracy: 0.7581\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7480\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1081 - custom_accuracy: 0.7493\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1077 - custom_accuracy: 0.7489\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1107 - custom_accuracy: 0.7586\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1091 - custom_accuracy: 0.7533\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1086 - custom_accuracy: 0.7515\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1085 - custom_accuracy: 0.7546\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1096 - custom_accuracy: 0.7482\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1033 - custom_accuracy: 0.7594\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1088 - custom_accuracy: 0.7502\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1117 - custom_accuracy: 0.7498\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1086 - custom_accuracy: 0.7533\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1095 - custom_accuracy: 0.7465\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1078 - custom_accuracy: 0.7526\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1077 - custom_accuracy: 0.7465\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1074 - custom_accuracy: 0.7607\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1112 - custom_accuracy: 0.7559\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1105 - custom_accuracy: 0.7544\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1089 - custom_accuracy: 0.7553\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1116 - custom_accuracy: 0.7533\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1102 - custom_accuracy: 0.7526\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1103 - custom_accuracy: 0.7632\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1073 - custom_accuracy: 0.7548\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1062 - custom_accuracy: 0.7511\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1080 - custom_accuracy: 0.7515\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1102 - custom_accuracy: 0.7566\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1092 - custom_accuracy: 0.7575\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1086 - custom_accuracy: 0.7579\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1067 - custom_accuracy: 0.7601\n",
            "Epoch 100/100\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 0.1106 - custom_accuracy: 0.7576{'loss': 0.10982484370470047, 'custom_accuracy': 0.7598681449890137}\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7599\n",
            "377\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_4 (Bidirecti  (None, 3, 20)             1200      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 20)                2480      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "76/76 [==============================] - 6s 7ms/step - loss: 0.2666 - custom_accuracy: 0.3546\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1489 - custom_accuracy: 0.5632\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1421 - custom_accuracy: 0.6618\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1395 - custom_accuracy: 0.6750\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1352 - custom_accuracy: 0.6836\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1356 - custom_accuracy: 0.6789\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1287 - custom_accuracy: 0.6842\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1265 - custom_accuracy: 0.6842\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1269 - custom_accuracy: 0.6730\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1235 - custom_accuracy: 0.6803\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 1s 9ms/step - loss: 0.1214 - custom_accuracy: 0.6822\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1234 - custom_accuracy: 0.6711\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1234 - custom_accuracy: 0.6783\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1215 - custom_accuracy: 0.6855\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1179 - custom_accuracy: 0.6980\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1178 - custom_accuracy: 0.7026\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1182 - custom_accuracy: 0.7026\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1181 - custom_accuracy: 0.6934\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1185 - custom_accuracy: 0.6921\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1173 - custom_accuracy: 0.7053\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1174 - custom_accuracy: 0.7243\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1152 - custom_accuracy: 0.7151\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1175 - custom_accuracy: 0.7013\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1169 - custom_accuracy: 0.7086\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1166 - custom_accuracy: 0.6993\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1146 - custom_accuracy: 0.6928\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1176 - custom_accuracy: 0.7112\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1149 - custom_accuracy: 0.7178\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1116 - custom_accuracy: 0.7293\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1161 - custom_accuracy: 0.7181\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1161 - custom_accuracy: 0.7105\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1151 - custom_accuracy: 0.7375\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1115 - custom_accuracy: 0.7063\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1162 - custom_accuracy: 0.7125\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1101 - custom_accuracy: 0.7230\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1173 - custom_accuracy: 0.6980\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1154 - custom_accuracy: 0.7072\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1146 - custom_accuracy: 0.7039\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1125 - custom_accuracy: 0.7250\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1153 - custom_accuracy: 0.7125\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1146 - custom_accuracy: 0.7276\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1133 - custom_accuracy: 0.7237\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1137 - custom_accuracy: 0.7243\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7326\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7224\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1099 - custom_accuracy: 0.7368\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1145 - custom_accuracy: 0.7164\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1094 - custom_accuracy: 0.7349\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1082 - custom_accuracy: 0.7303\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1095 - custom_accuracy: 0.7388\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1128 - custom_accuracy: 0.7401\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1093 - custom_accuracy: 0.7237\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1115 - custom_accuracy: 0.7500\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1108 - custom_accuracy: 0.7349\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1102 - custom_accuracy: 0.7401\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1119 - custom_accuracy: 0.7309\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1101 - custom_accuracy: 0.7592\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1110 - custom_accuracy: 0.7414\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1084 - custom_accuracy: 0.7401\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1091 - custom_accuracy: 0.7480\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1106 - custom_accuracy: 0.7408\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1090 - custom_accuracy: 0.7526\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1051 - custom_accuracy: 0.7408\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1125 - custom_accuracy: 0.7296\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1087 - custom_accuracy: 0.7474\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1046 - custom_accuracy: 0.7645\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1098 - custom_accuracy: 0.7467\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1022 - custom_accuracy: 0.7711\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1059 - custom_accuracy: 0.7796\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1097 - custom_accuracy: 0.7553\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1071 - custom_accuracy: 0.7645\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1037 - custom_accuracy: 0.7684\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1083 - custom_accuracy: 0.7730\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1047 - custom_accuracy: 0.7658\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1053 - custom_accuracy: 0.7743\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1080 - custom_accuracy: 0.7618\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1040 - custom_accuracy: 0.7658\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1072 - custom_accuracy: 0.7618\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1060 - custom_accuracy: 0.7773\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1071 - custom_accuracy: 0.7711\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1026 - custom_accuracy: 0.7809\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1024 - custom_accuracy: 0.7816\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1051 - custom_accuracy: 0.7757\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1051 - custom_accuracy: 0.7730\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1049 - custom_accuracy: 0.7836\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1056 - custom_accuracy: 0.7816\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1054 - custom_accuracy: 0.7743\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1044 - custom_accuracy: 0.7763\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1017 - custom_accuracy: 0.7737\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1042 - custom_accuracy: 0.7743\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1037 - custom_accuracy: 0.7757\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1050 - custom_accuracy: 0.7822\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1056 - custom_accuracy: 0.7747\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1012 - custom_accuracy: 0.7829\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1030 - custom_accuracy: 0.7750\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1072 - custom_accuracy: 0.7714\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1006 - custom_accuracy: 0.7737\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1002 - custom_accuracy: 0.7849\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1056 - custom_accuracy: 0.7901\n",
            "Epoch 100/100\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 0.1011 - custom_accuracy: 0.7833{'loss': 0.10063966363668442, 'custom_accuracy': 0.7888158559799194}\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1006 - custom_accuracy: 0.7888\n",
            "375\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_6 (Bidirecti  (None, 5, 20)             1200      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, 20)                2480      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "75/75 [==============================] - 5s 6ms/step - loss: 0.2287 - custom_accuracy: 0.4040\n",
            "Epoch 2/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1632 - custom_accuracy: 0.5567\n",
            "Epoch 3/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1559 - custom_accuracy: 0.6067\n",
            "Epoch 4/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1412 - custom_accuracy: 0.6347\n",
            "Epoch 5/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1391 - custom_accuracy: 0.6493\n",
            "Epoch 6/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1375 - custom_accuracy: 0.6627\n",
            "Epoch 7/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1310 - custom_accuracy: 0.6780\n",
            "Epoch 8/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1297 - custom_accuracy: 0.6627\n",
            "Epoch 9/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1270 - custom_accuracy: 0.6840\n",
            "Epoch 10/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1289 - custom_accuracy: 0.6767\n",
            "Epoch 11/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1247 - custom_accuracy: 0.6673\n",
            "Epoch 12/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1249 - custom_accuracy: 0.6800\n",
            "Epoch 13/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1228 - custom_accuracy: 0.6733\n",
            "Epoch 14/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1241 - custom_accuracy: 0.6827\n",
            "Epoch 15/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1213 - custom_accuracy: 0.6640\n",
            "Epoch 16/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1177 - custom_accuracy: 0.6933\n",
            "Epoch 17/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1195 - custom_accuracy: 0.6887\n",
            "Epoch 18/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1170 - custom_accuracy: 0.6953\n",
            "Epoch 19/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1165 - custom_accuracy: 0.7173\n",
            "Epoch 20/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1206 - custom_accuracy: 0.6813\n",
            "Epoch 21/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1165 - custom_accuracy: 0.7187\n",
            "Epoch 22/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1194 - custom_accuracy: 0.6933\n",
            "Epoch 23/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1159 - custom_accuracy: 0.6967\n",
            "Epoch 24/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1179 - custom_accuracy: 0.7060\n",
            "Epoch 25/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1164 - custom_accuracy: 0.6840\n",
            "Epoch 26/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1152 - custom_accuracy: 0.6993\n",
            "Epoch 27/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1217 - custom_accuracy: 0.6727\n",
            "Epoch 28/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1110 - custom_accuracy: 0.7073\n",
            "Epoch 29/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1145 - custom_accuracy: 0.7227\n",
            "Epoch 30/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1122 - custom_accuracy: 0.7113\n",
            "Epoch 31/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1110 - custom_accuracy: 0.7153\n",
            "Epoch 32/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1115 - custom_accuracy: 0.7400\n",
            "Epoch 33/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1145 - custom_accuracy: 0.7013\n",
            "Epoch 34/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1105 - custom_accuracy: 0.7313\n",
            "Epoch 35/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1104 - custom_accuracy: 0.7320\n",
            "Epoch 36/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1097 - custom_accuracy: 0.7087\n",
            "Epoch 37/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1071 - custom_accuracy: 0.7333\n",
            "Epoch 38/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1091 - custom_accuracy: 0.7147\n",
            "Epoch 39/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1066 - custom_accuracy: 0.7520\n",
            "Epoch 40/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1086 - custom_accuracy: 0.7327\n",
            "Epoch 41/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1064 - custom_accuracy: 0.7420\n",
            "Epoch 42/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1060 - custom_accuracy: 0.7413\n",
            "Epoch 43/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1041 - custom_accuracy: 0.7480\n",
            "Epoch 44/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1052 - custom_accuracy: 0.7467\n",
            "Epoch 45/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1020 - custom_accuracy: 0.7587\n",
            "Epoch 46/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1039 - custom_accuracy: 0.7627\n",
            "Epoch 47/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1034 - custom_accuracy: 0.7493\n",
            "Epoch 48/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1050 - custom_accuracy: 0.7573\n",
            "Epoch 49/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1013 - custom_accuracy: 0.7660\n",
            "Epoch 50/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0974 - custom_accuracy: 0.7740\n",
            "Epoch 51/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1002 - custom_accuracy: 0.7793\n",
            "Epoch 52/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1043 - custom_accuracy: 0.7693\n",
            "Epoch 53/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0994 - custom_accuracy: 0.7813\n",
            "Epoch 54/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0976 - custom_accuracy: 0.7773\n",
            "Epoch 55/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1001 - custom_accuracy: 0.7807\n",
            "Epoch 56/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0977 - custom_accuracy: 0.7947\n",
            "Epoch 57/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1001 - custom_accuracy: 0.7893\n",
            "Epoch 58/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0976 - custom_accuracy: 0.7860\n",
            "Epoch 59/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0966 - custom_accuracy: 0.7873\n",
            "Epoch 60/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0948 - custom_accuracy: 0.7973\n",
            "Epoch 61/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0957 - custom_accuracy: 0.7973\n",
            "Epoch 62/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0938 - custom_accuracy: 0.7867\n",
            "Epoch 63/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0955 - custom_accuracy: 0.7900\n",
            "Epoch 64/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0950 - custom_accuracy: 0.8100\n",
            "Epoch 65/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0971 - custom_accuracy: 0.7980\n",
            "Epoch 66/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0949 - custom_accuracy: 0.7973\n",
            "Epoch 67/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0949 - custom_accuracy: 0.7953\n",
            "Epoch 68/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0943 - custom_accuracy: 0.8020\n",
            "Epoch 69/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0922 - custom_accuracy: 0.8013\n",
            "Epoch 70/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0934 - custom_accuracy: 0.8020\n",
            "Epoch 71/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0927 - custom_accuracy: 0.8040\n",
            "Epoch 72/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0945 - custom_accuracy: 0.8047\n",
            "Epoch 73/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0926 - custom_accuracy: 0.8127\n",
            "Epoch 74/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0923 - custom_accuracy: 0.8013\n",
            "Epoch 75/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0907 - custom_accuracy: 0.8047\n",
            "Epoch 76/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0907 - custom_accuracy: 0.8087\n",
            "Epoch 77/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0912 - custom_accuracy: 0.8127\n",
            "Epoch 78/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0918 - custom_accuracy: 0.8100\n",
            "Epoch 79/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0934 - custom_accuracy: 0.7953\n",
            "Epoch 80/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0905 - custom_accuracy: 0.8153\n",
            "Epoch 81/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0877 - custom_accuracy: 0.8073\n",
            "Epoch 82/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0918 - custom_accuracy: 0.8147\n",
            "Epoch 83/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0895 - custom_accuracy: 0.8100\n",
            "Epoch 84/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0880 - custom_accuracy: 0.8227\n",
            "Epoch 85/100\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.0902 - custom_accuracy: 0.8107\n",
            "Epoch 86/100\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.0885 - custom_accuracy: 0.8167\n",
            "Epoch 87/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0890 - custom_accuracy: 0.8160\n",
            "Epoch 88/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0887 - custom_accuracy: 0.8147\n",
            "Epoch 89/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0879 - custom_accuracy: 0.8173\n",
            "Epoch 90/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0873 - custom_accuracy: 0.8200\n",
            "Epoch 91/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0853 - custom_accuracy: 0.8213\n",
            "Epoch 92/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0863 - custom_accuracy: 0.8207\n",
            "Epoch 93/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0857 - custom_accuracy: 0.8093\n",
            "Epoch 94/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0895 - custom_accuracy: 0.8267\n",
            "Epoch 95/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0863 - custom_accuracy: 0.8267\n",
            "Epoch 96/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0854 - custom_accuracy: 0.8307\n",
            "Epoch 97/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0852 - custom_accuracy: 0.8220\n",
            "Epoch 98/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0871 - custom_accuracy: 0.8347\n",
            "Epoch 99/100\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0844 - custom_accuracy: 0.8300\n",
            "Epoch 100/100\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0885 - custom_accuracy: 0.8413{'loss': 0.08766112476587296, 'custom_accuracy': 0.8439997434616089}\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0877 - custom_accuracy: 0.8440\n",
            "370\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_8 (Bidirecti  (None, 10, 20)            1200      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirecti  (None, 20)                2480      \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "74/74 [==============================] - 5s 7ms/step - loss: 0.2630 - custom_accuracy: 0.3277\n",
            "Epoch 2/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1777 - custom_accuracy: 0.4865\n",
            "Epoch 3/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1685 - custom_accuracy: 0.5574\n",
            "Epoch 4/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1673 - custom_accuracy: 0.5507\n",
            "Epoch 5/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1589 - custom_accuracy: 0.5838\n",
            "Epoch 6/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1509 - custom_accuracy: 0.6027\n",
            "Epoch 7/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1555 - custom_accuracy: 0.6216\n",
            "Epoch 8/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1479 - custom_accuracy: 0.5993\n",
            "Epoch 9/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1422 - custom_accuracy: 0.6547\n",
            "Epoch 10/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1385 - custom_accuracy: 0.6662\n",
            "Epoch 11/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1428 - custom_accuracy: 0.6426\n",
            "Epoch 12/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1322 - custom_accuracy: 0.6723\n",
            "Epoch 13/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1287 - custom_accuracy: 0.6804\n",
            "Epoch 14/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1310 - custom_accuracy: 0.7034\n",
            "Epoch 15/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1261 - custom_accuracy: 0.7034\n",
            "Epoch 16/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1280 - custom_accuracy: 0.6764\n",
            "Epoch 17/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1253 - custom_accuracy: 0.7007\n",
            "Epoch 18/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1229 - custom_accuracy: 0.7041\n",
            "Epoch 19/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1224 - custom_accuracy: 0.7007\n",
            "Epoch 20/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1185 - custom_accuracy: 0.7243\n",
            "Epoch 21/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1176 - custom_accuracy: 0.7284\n",
            "Epoch 22/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1128 - custom_accuracy: 0.7392\n",
            "Epoch 23/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1147 - custom_accuracy: 0.7385\n",
            "Epoch 24/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1137 - custom_accuracy: 0.7392\n",
            "Epoch 25/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1174 - custom_accuracy: 0.7007\n",
            "Epoch 26/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1099 - custom_accuracy: 0.7459\n",
            "Epoch 27/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1132 - custom_accuracy: 0.7399\n",
            "Epoch 28/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1147 - custom_accuracy: 0.7264\n",
            "Epoch 29/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1078 - custom_accuracy: 0.7534\n",
            "Epoch 30/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1103 - custom_accuracy: 0.7628\n",
            "Epoch 31/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1092 - custom_accuracy: 0.7588\n",
            "Epoch 32/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1064 - custom_accuracy: 0.7439\n",
            "Epoch 33/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1085 - custom_accuracy: 0.7419\n",
            "Epoch 34/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1061 - custom_accuracy: 0.7277\n",
            "Epoch 35/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1072 - custom_accuracy: 0.7635\n",
            "Epoch 36/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1095 - custom_accuracy: 0.7547\n",
            "Epoch 37/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1054 - custom_accuracy: 0.7541\n",
            "Epoch 38/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1062 - custom_accuracy: 0.7561\n",
            "Epoch 39/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1046 - custom_accuracy: 0.7635\n",
            "Epoch 40/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1050 - custom_accuracy: 0.7541\n",
            "Epoch 41/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1039 - custom_accuracy: 0.7466\n",
            "Epoch 42/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1060 - custom_accuracy: 0.7547\n",
            "Epoch 43/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1020 - custom_accuracy: 0.7703\n",
            "Epoch 44/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1042 - custom_accuracy: 0.7635\n",
            "Epoch 45/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1041 - custom_accuracy: 0.7480\n",
            "Epoch 46/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1034 - custom_accuracy: 0.7655\n",
            "Epoch 47/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1000 - custom_accuracy: 0.7696\n",
            "Epoch 48/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0997 - custom_accuracy: 0.7730\n",
            "Epoch 49/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1008 - custom_accuracy: 0.7682\n",
            "Epoch 50/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1013 - custom_accuracy: 0.7743\n",
            "Epoch 51/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1008 - custom_accuracy: 0.7703\n",
            "Epoch 52/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0953 - custom_accuracy: 0.7818\n",
            "Epoch 53/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1018 - custom_accuracy: 0.7669\n",
            "Epoch 54/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0996 - custom_accuracy: 0.7791\n",
            "Epoch 55/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0979 - custom_accuracy: 0.7872\n",
            "Epoch 56/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0961 - custom_accuracy: 0.7858\n",
            "Epoch 57/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0971 - custom_accuracy: 0.7926\n",
            "Epoch 58/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0937 - custom_accuracy: 0.7865\n",
            "Epoch 59/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0978 - custom_accuracy: 0.7878\n",
            "Epoch 60/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0934 - custom_accuracy: 0.7980\n",
            "Epoch 61/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0985 - custom_accuracy: 0.7919\n",
            "Epoch 62/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0958 - custom_accuracy: 0.7838\n",
            "Epoch 63/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0954 - custom_accuracy: 0.7993\n",
            "Epoch 64/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0943 - custom_accuracy: 0.7953\n",
            "Epoch 65/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0937 - custom_accuracy: 0.8020\n",
            "Epoch 66/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0921 - custom_accuracy: 0.8047\n",
            "Epoch 67/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0921 - custom_accuracy: 0.8149\n",
            "Epoch 68/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0913 - custom_accuracy: 0.8074\n",
            "Epoch 69/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0925 - custom_accuracy: 0.8020\n",
            "Epoch 70/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0911 - custom_accuracy: 0.8176\n",
            "Epoch 71/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0924 - custom_accuracy: 0.8027\n",
            "Epoch 72/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0892 - custom_accuracy: 0.8054\n",
            "Epoch 73/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0909 - custom_accuracy: 0.8155\n",
            "Epoch 74/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0925 - custom_accuracy: 0.8108\n",
            "Epoch 75/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0897 - custom_accuracy: 0.8189\n",
            "Epoch 76/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0883 - custom_accuracy: 0.8189\n",
            "Epoch 77/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0902 - custom_accuracy: 0.8223\n",
            "Epoch 78/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0906 - custom_accuracy: 0.8095\n",
            "Epoch 79/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0903 - custom_accuracy: 0.8176\n",
            "Epoch 80/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0874 - custom_accuracy: 0.8277\n",
            "Epoch 81/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0890 - custom_accuracy: 0.8236\n",
            "Epoch 82/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0876 - custom_accuracy: 0.8135\n",
            "Epoch 83/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0882 - custom_accuracy: 0.8331\n",
            "Epoch 84/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0870 - custom_accuracy: 0.8277\n",
            "Epoch 85/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0913 - custom_accuracy: 0.8128\n",
            "Epoch 86/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0886 - custom_accuracy: 0.8243\n",
            "Epoch 87/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0857 - custom_accuracy: 0.8277\n",
            "Epoch 88/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0878 - custom_accuracy: 0.8128\n",
            "Epoch 89/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0849 - custom_accuracy: 0.8365\n",
            "Epoch 90/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0855 - custom_accuracy: 0.8270\n",
            "Epoch 91/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0851 - custom_accuracy: 0.8209\n",
            "Epoch 92/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0860 - custom_accuracy: 0.8351\n",
            "Epoch 93/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0849 - custom_accuracy: 0.8318\n",
            "Epoch 94/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0835 - custom_accuracy: 0.8264\n",
            "Epoch 95/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0841 - custom_accuracy: 0.8257\n",
            "Epoch 96/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0886 - custom_accuracy: 0.8203\n",
            "Epoch 97/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.0863 - custom_accuracy: 0.8351\n",
            "Epoch 98/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0857 - custom_accuracy: 0.8297\n",
            "Epoch 99/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0851 - custom_accuracy: 0.8291\n",
            "Epoch 100/100\n",
            "73/74 [============================>.] - ETA: 0s - loss: 0.0856 - custom_accuracy: 0.8301{'loss': 0.08489806205034256, 'custom_accuracy': 0.832432210445404}\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.0849 - custom_accuracy: 0.8324\n",
            "365\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_10 (Bidirect  (None, 15, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 7ms/step - loss: 0.2235 - custom_accuracy: 0.3993\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1773 - custom_accuracy: 0.5308\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1728 - custom_accuracy: 0.5336\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1667 - custom_accuracy: 0.5801\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1615 - custom_accuracy: 0.5795\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1701 - custom_accuracy: 0.5247\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1585 - custom_accuracy: 0.5870\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1516 - custom_accuracy: 0.6192\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1488 - custom_accuracy: 0.6253\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1400 - custom_accuracy: 0.6370\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1453 - custom_accuracy: 0.6130\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1311 - custom_accuracy: 0.6904\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1242 - custom_accuracy: 0.6719\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1236 - custom_accuracy: 0.7000\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1131 - custom_accuracy: 0.7336\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1144 - custom_accuracy: 0.7425\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1125 - custom_accuracy: 0.7247\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1124 - custom_accuracy: 0.7342\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.1139 - custom_accuracy: 0.7349\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 1s 9ms/step - loss: 0.1095 - custom_accuracy: 0.7596\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1093 - custom_accuracy: 0.7555\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1089 - custom_accuracy: 0.7603\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1116 - custom_accuracy: 0.7295\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1080 - custom_accuracy: 0.7596\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1066 - custom_accuracy: 0.7671\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1085 - custom_accuracy: 0.7562\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1052 - custom_accuracy: 0.7623\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1049 - custom_accuracy: 0.7630\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1101 - custom_accuracy: 0.7425\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1069 - custom_accuracy: 0.7582\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.1058 - custom_accuracy: 0.7712\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1022 - custom_accuracy: 0.7760\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1016 - custom_accuracy: 0.7664\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1036 - custom_accuracy: 0.7801\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1018 - custom_accuracy: 0.7911\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1014 - custom_accuracy: 0.7795\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1025 - custom_accuracy: 0.7781\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0998 - custom_accuracy: 0.7856\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0989 - custom_accuracy: 0.7801\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.1032 - custom_accuracy: 0.7685\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0982 - custom_accuracy: 0.7925\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0977 - custom_accuracy: 0.7890\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0974 - custom_accuracy: 0.7945\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0987 - custom_accuracy: 0.7890\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0977 - custom_accuracy: 0.8055\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0972 - custom_accuracy: 0.7973\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0960 - custom_accuracy: 0.8000\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0983 - custom_accuracy: 0.7836\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0956 - custom_accuracy: 0.7993\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0952 - custom_accuracy: 0.8089\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0951 - custom_accuracy: 0.7993\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0932 - custom_accuracy: 0.8130\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0941 - custom_accuracy: 0.8137\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0947 - custom_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0954 - custom_accuracy: 0.8021\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0923 - custom_accuracy: 0.8103\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0926 - custom_accuracy: 0.8137\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0920 - custom_accuracy: 0.8007\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0922 - custom_accuracy: 0.8103\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0910 - custom_accuracy: 0.8185\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0907 - custom_accuracy: 0.8103\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0914 - custom_accuracy: 0.8075\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0922 - custom_accuracy: 0.8178\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0914 - custom_accuracy: 0.8212\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0916 - custom_accuracy: 0.8260\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0911 - custom_accuracy: 0.8212\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0908 - custom_accuracy: 0.8171\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0897 - custom_accuracy: 0.8288\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0898 - custom_accuracy: 0.8151\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0912 - custom_accuracy: 0.8103\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0901 - custom_accuracy: 0.8212\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0874 - custom_accuracy: 0.8253\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0875 - custom_accuracy: 0.8301\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0914 - custom_accuracy: 0.8363\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0880 - custom_accuracy: 0.8178\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0881 - custom_accuracy: 0.8253\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0870 - custom_accuracy: 0.8390\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0881 - custom_accuracy: 0.8288\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0889 - custom_accuracy: 0.8288\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0862 - custom_accuracy: 0.8233\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0876 - custom_accuracy: 0.8336\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0880 - custom_accuracy: 0.8349\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0877 - custom_accuracy: 0.8308\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0862 - custom_accuracy: 0.8356\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0879 - custom_accuracy: 0.8411\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0890 - custom_accuracy: 0.8247\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0850 - custom_accuracy: 0.8425\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0881 - custom_accuracy: 0.8274\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0857 - custom_accuracy: 0.8267\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0880 - custom_accuracy: 0.8295\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0864 - custom_accuracy: 0.8308\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0864 - custom_accuracy: 0.8418\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0860 - custom_accuracy: 0.8356\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0850 - custom_accuracy: 0.8438\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0829 - custom_accuracy: 0.8390\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0853 - custom_accuracy: 0.8356\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0845 - custom_accuracy: 0.8404\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0834 - custom_accuracy: 0.8500\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.0838 - custom_accuracy: 0.8432\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - ETA: 0s - loss: 0.0838 - custom_accuracy: 0.8377{'loss': 0.08380270004272461, 'custom_accuracy': 0.837671160697937}\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.0838 - custom_accuracy: 0.8377\n",
            "355\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_12 (Bidirect  (None, 25, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_13 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "71/71 [==============================] - 5s 8ms/step - loss: 0.2703 - custom_accuracy: 0.3035\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1860 - custom_accuracy: 0.4415\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1769 - custom_accuracy: 0.5099\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1703 - custom_accuracy: 0.5317\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1619 - custom_accuracy: 0.5761\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1555 - custom_accuracy: 0.5915\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1528 - custom_accuracy: 0.6120\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1488 - custom_accuracy: 0.6162\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1410 - custom_accuracy: 0.6331\n",
            "Epoch 10/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1385 - custom_accuracy: 0.6563\n",
            "Epoch 11/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1319 - custom_accuracy: 0.6599\n",
            "Epoch 12/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1255 - custom_accuracy: 0.6866\n",
            "Epoch 13/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1263 - custom_accuracy: 0.6859\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1229 - custom_accuracy: 0.6937\n",
            "Epoch 15/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1199 - custom_accuracy: 0.6993\n",
            "Epoch 16/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.1158 - custom_accuracy: 0.7197\n",
            "Epoch 17/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.1172 - custom_accuracy: 0.7197\n",
            "Epoch 18/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1146 - custom_accuracy: 0.7261\n",
            "Epoch 19/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1137 - custom_accuracy: 0.7324\n",
            "Epoch 20/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1132 - custom_accuracy: 0.7437\n",
            "Epoch 21/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1103 - custom_accuracy: 0.7451\n",
            "Epoch 22/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1090 - custom_accuracy: 0.7507\n",
            "Epoch 23/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1115 - custom_accuracy: 0.7423\n",
            "Epoch 24/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1085 - custom_accuracy: 0.7690\n",
            "Epoch 25/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1082 - custom_accuracy: 0.7606\n",
            "Epoch 26/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1085 - custom_accuracy: 0.7556\n",
            "Epoch 27/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1042 - custom_accuracy: 0.7641\n",
            "Epoch 28/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1064 - custom_accuracy: 0.7451\n",
            "Epoch 29/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1078 - custom_accuracy: 0.7592\n",
            "Epoch 30/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1107 - custom_accuracy: 0.7331\n",
            "Epoch 31/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1056 - custom_accuracy: 0.7711\n",
            "Epoch 32/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1055 - custom_accuracy: 0.7592\n",
            "Epoch 33/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1047 - custom_accuracy: 0.7718\n",
            "Epoch 34/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1045 - custom_accuracy: 0.7704\n",
            "Epoch 35/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1004 - custom_accuracy: 0.7810\n",
            "Epoch 36/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1063 - custom_accuracy: 0.7627\n",
            "Epoch 37/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.1049 - custom_accuracy: 0.7599\n",
            "Epoch 38/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1051 - custom_accuracy: 0.7725\n",
            "Epoch 39/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1029 - custom_accuracy: 0.7704\n",
            "Epoch 40/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1020 - custom_accuracy: 0.7796\n",
            "Epoch 41/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1031 - custom_accuracy: 0.7838\n",
            "Epoch 42/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1040 - custom_accuracy: 0.7697\n",
            "Epoch 43/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1057 - custom_accuracy: 0.7683\n",
            "Epoch 44/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0993 - custom_accuracy: 0.7775\n",
            "Epoch 45/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1005 - custom_accuracy: 0.7768\n",
            "Epoch 46/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.1004 - custom_accuracy: 0.7901\n",
            "Epoch 47/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0985 - custom_accuracy: 0.7901\n",
            "Epoch 48/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0983 - custom_accuracy: 0.7930\n",
            "Epoch 49/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0986 - custom_accuracy: 0.7831\n",
            "Epoch 50/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0975 - custom_accuracy: 0.7951\n",
            "Epoch 51/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0962 - custom_accuracy: 0.7972\n",
            "Epoch 52/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0959 - custom_accuracy: 0.7923\n",
            "Epoch 53/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0953 - custom_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0975 - custom_accuracy: 0.8092\n",
            "Epoch 55/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0940 - custom_accuracy: 0.8077\n",
            "Epoch 56/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.0965 - custom_accuracy: 0.8092\n",
            "Epoch 57/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0934 - custom_accuracy: 0.7944\n",
            "Epoch 58/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0915 - custom_accuracy: 0.8141\n",
            "Epoch 59/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0912 - custom_accuracy: 0.8021\n",
            "Epoch 60/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0907 - custom_accuracy: 0.8113\n",
            "Epoch 61/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0913 - custom_accuracy: 0.8134\n",
            "Epoch 62/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0899 - custom_accuracy: 0.8120\n",
            "Epoch 63/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0908 - custom_accuracy: 0.8183\n",
            "Epoch 64/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0908 - custom_accuracy: 0.8268\n",
            "Epoch 65/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0870 - custom_accuracy: 0.8176\n",
            "Epoch 66/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0871 - custom_accuracy: 0.8246\n",
            "Epoch 67/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0874 - custom_accuracy: 0.8113\n",
            "Epoch 68/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0856 - custom_accuracy: 0.8289\n",
            "Epoch 69/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0832 - custom_accuracy: 0.8338\n",
            "Epoch 70/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0864 - custom_accuracy: 0.8331\n",
            "Epoch 71/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0854 - custom_accuracy: 0.8401\n",
            "Epoch 72/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0847 - custom_accuracy: 0.8275\n",
            "Epoch 73/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0844 - custom_accuracy: 0.8331\n",
            "Epoch 74/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0820 - custom_accuracy: 0.8380\n",
            "Epoch 75/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0833 - custom_accuracy: 0.8380\n",
            "Epoch 76/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.0821 - custom_accuracy: 0.8394\n",
            "Epoch 77/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.0876 - custom_accuracy: 0.8169\n",
            "Epoch 78/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0837 - custom_accuracy: 0.8324\n",
            "Epoch 79/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0820 - custom_accuracy: 0.8345\n",
            "Epoch 80/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0815 - custom_accuracy: 0.8408\n",
            "Epoch 81/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0823 - custom_accuracy: 0.8486\n",
            "Epoch 82/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0834 - custom_accuracy: 0.8387\n",
            "Epoch 83/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0832 - custom_accuracy: 0.8415\n",
            "Epoch 84/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0810 - custom_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0819 - custom_accuracy: 0.8514\n",
            "Epoch 86/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0805 - custom_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0809 - custom_accuracy: 0.8528\n",
            "Epoch 88/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0778 - custom_accuracy: 0.8606\n",
            "Epoch 89/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0805 - custom_accuracy: 0.8570\n",
            "Epoch 90/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0807 - custom_accuracy: 0.8451\n",
            "Epoch 91/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0776 - custom_accuracy: 0.8620\n",
            "Epoch 92/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0783 - custom_accuracy: 0.8542\n",
            "Epoch 93/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0780 - custom_accuracy: 0.8486\n",
            "Epoch 94/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0804 - custom_accuracy: 0.8549\n",
            "Epoch 95/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0791 - custom_accuracy: 0.8592\n",
            "Epoch 96/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.0810 - custom_accuracy: 0.8458\n",
            "Epoch 97/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0795 - custom_accuracy: 0.8599\n",
            "Epoch 98/100\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 0.0785 - custom_accuracy: 0.8585\n",
            "Epoch 99/100\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0776 - custom_accuracy: 0.8627\n",
            "Epoch 100/100\n",
            "70/71 [============================>.] - ETA: 0s - loss: 0.0758 - custom_accuracy: 0.8729{'loss': 0.07609807699918747, 'custom_accuracy': 0.8718308806419373}\n",
            "71/71 [==============================] - 1s 8ms/step - loss: 0.0761 - custom_accuracy: 0.8718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length_variations = [30, 35, 40, 45, 50, 55]\n",
        "for i in range(len(seq_length_variations)):\n",
        "  seq_length = seq_length_variations[i]\n",
        "  X_train, y_train = create_sequences(df_train_scaled, seq_length)\n",
        "  model_lstm = Sequential()\n",
        "  model_lstm.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(seq_length, 4)))\n",
        "  model_lstm.add(Bidirectional(LSTM(10)))\n",
        "  model_lstm.add(Dense(4))  # Output layer has 4 units to predict all features\n",
        "\n",
        "  model_lstm.compile(optimizer='adam', loss=rmse, metrics=[custom_accuracy])\n",
        "  model_lstm.summary()\n",
        "  model_lstm.fit(X_train, y_train, epochs=100, batch_size=5, verbose = 0, callbacks=[PrintLastEpoch()])\n",
        "  models.append(model_lstm)\n",
        "  last_trains.append(X_train[len(X_train) - 1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDxMQZiexRhM",
        "outputId": "9f976f63-82a1-4c7e-94d6-0da9ef6affcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_14 (Bidirect  (None, 30, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_15 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "70/70 [==============================] - 6s 9ms/step - loss: 0.2194 - custom_accuracy: 0.3850\n",
            "Epoch 2/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1733 - custom_accuracy: 0.5036\n",
            "Epoch 3/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1580 - custom_accuracy: 0.5586\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1540 - custom_accuracy: 0.5929\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1437 - custom_accuracy: 0.6221\n",
            "Epoch 6/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1382 - custom_accuracy: 0.6179\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1338 - custom_accuracy: 0.6271\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1337 - custom_accuracy: 0.6279\n",
            "Epoch 9/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1282 - custom_accuracy: 0.6114\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1220 - custom_accuracy: 0.6293\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1207 - custom_accuracy: 0.6700\n",
            "Epoch 12/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1145 - custom_accuracy: 0.6757\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1150 - custom_accuracy: 0.6614\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1095 - custom_accuracy: 0.6900\n",
            "Epoch 15/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1084 - custom_accuracy: 0.6871\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1053 - custom_accuracy: 0.6993\n",
            "Epoch 17/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1094 - custom_accuracy: 0.6929\n",
            "Epoch 18/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1071 - custom_accuracy: 0.6871\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1039 - custom_accuracy: 0.7171\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1057 - custom_accuracy: 0.7157\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1038 - custom_accuracy: 0.7207\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1060 - custom_accuracy: 0.7186\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1050 - custom_accuracy: 0.7193\n",
            "Epoch 24/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1012 - custom_accuracy: 0.7286\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1001 - custom_accuracy: 0.7443\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1005 - custom_accuracy: 0.7321\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1003 - custom_accuracy: 0.7464\n",
            "Epoch 28/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1015 - custom_accuracy: 0.7286\n",
            "Epoch 29/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1008 - custom_accuracy: 0.7379\n",
            "Epoch 30/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0976 - custom_accuracy: 0.7493\n",
            "Epoch 31/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1008 - custom_accuracy: 0.7464\n",
            "Epoch 32/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.1007 - custom_accuracy: 0.7521\n",
            "Epoch 33/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0979 - custom_accuracy: 0.7521\n",
            "Epoch 34/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0974 - custom_accuracy: 0.7571\n",
            "Epoch 35/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0972 - custom_accuracy: 0.7636\n",
            "Epoch 36/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0966 - custom_accuracy: 0.7564\n",
            "Epoch 37/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.1007 - custom_accuracy: 0.7271\n",
            "Epoch 38/100\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0986 - custom_accuracy: 0.7529\n",
            "Epoch 39/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0983 - custom_accuracy: 0.7536\n",
            "Epoch 40/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0993 - custom_accuracy: 0.7307\n",
            "Epoch 41/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0972 - custom_accuracy: 0.7579\n",
            "Epoch 42/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0960 - custom_accuracy: 0.7607\n",
            "Epoch 43/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0952 - custom_accuracy: 0.7543\n",
            "Epoch 44/100\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0966 - custom_accuracy: 0.7536\n",
            "Epoch 45/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0946 - custom_accuracy: 0.7643\n",
            "Epoch 46/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0929 - custom_accuracy: 0.7586\n",
            "Epoch 47/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0939 - custom_accuracy: 0.7729\n",
            "Epoch 48/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0955 - custom_accuracy: 0.7836\n",
            "Epoch 49/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0944 - custom_accuracy: 0.7586\n",
            "Epoch 50/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0942 - custom_accuracy: 0.7714\n",
            "Epoch 51/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0910 - custom_accuracy: 0.7864\n",
            "Epoch 52/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0926 - custom_accuracy: 0.7957\n",
            "Epoch 53/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0892 - custom_accuracy: 0.7757\n",
            "Epoch 54/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0933 - custom_accuracy: 0.7786\n",
            "Epoch 55/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0892 - custom_accuracy: 0.7857\n",
            "Epoch 56/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0921 - custom_accuracy: 0.7836\n",
            "Epoch 57/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0892 - custom_accuracy: 0.7843\n",
            "Epoch 58/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0927 - custom_accuracy: 0.7836\n",
            "Epoch 59/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0888 - custom_accuracy: 0.7971\n",
            "Epoch 60/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0924 - custom_accuracy: 0.7779\n",
            "Epoch 61/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0885 - custom_accuracy: 0.7957\n",
            "Epoch 62/100\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0875 - custom_accuracy: 0.7871\n",
            "Epoch 63/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0893 - custom_accuracy: 0.7971\n",
            "Epoch 64/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0885 - custom_accuracy: 0.7893\n",
            "Epoch 65/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0858 - custom_accuracy: 0.7993\n",
            "Epoch 66/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0898 - custom_accuracy: 0.7957\n",
            "Epoch 67/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0879 - custom_accuracy: 0.8029\n",
            "Epoch 68/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0878 - custom_accuracy: 0.7971\n",
            "Epoch 69/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0877 - custom_accuracy: 0.8014\n",
            "Epoch 70/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0883 - custom_accuracy: 0.8021\n",
            "Epoch 71/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0860 - custom_accuracy: 0.8079\n",
            "Epoch 72/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0863 - custom_accuracy: 0.8057\n",
            "Epoch 73/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0873 - custom_accuracy: 0.7929\n",
            "Epoch 74/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0879 - custom_accuracy: 0.7979\n",
            "Epoch 75/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0898 - custom_accuracy: 0.7743\n",
            "Epoch 76/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0863 - custom_accuracy: 0.8093\n",
            "Epoch 77/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0859 - custom_accuracy: 0.8021\n",
            "Epoch 78/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0852 - custom_accuracy: 0.8043\n",
            "Epoch 79/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0885 - custom_accuracy: 0.8071\n",
            "Epoch 80/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0852 - custom_accuracy: 0.8050\n",
            "Epoch 81/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0851 - custom_accuracy: 0.8107\n",
            "Epoch 82/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0874 - custom_accuracy: 0.8064\n",
            "Epoch 83/100\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0847 - custom_accuracy: 0.8064\n",
            "Epoch 84/100\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0854 - custom_accuracy: 0.8136\n",
            "Epoch 85/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0854 - custom_accuracy: 0.8214\n",
            "Epoch 86/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0840 - custom_accuracy: 0.8100\n",
            "Epoch 87/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0869 - custom_accuracy: 0.8000\n",
            "Epoch 88/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0842 - custom_accuracy: 0.8193\n",
            "Epoch 89/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0824 - custom_accuracy: 0.8193\n",
            "Epoch 90/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0869 - custom_accuracy: 0.8071\n",
            "Epoch 91/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0842 - custom_accuracy: 0.8171\n",
            "Epoch 92/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0818 - custom_accuracy: 0.8236\n",
            "Epoch 93/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0837 - custom_accuracy: 0.8136\n",
            "Epoch 94/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0791 - custom_accuracy: 0.8200\n",
            "Epoch 95/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0823 - custom_accuracy: 0.8193\n",
            "Epoch 96/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0828 - custom_accuracy: 0.8179\n",
            "Epoch 97/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0839 - custom_accuracy: 0.8186\n",
            "Epoch 98/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0832 - custom_accuracy: 0.8143\n",
            "Epoch 99/100\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0799 - custom_accuracy: 0.8171\n",
            "Epoch 100/100\n",
            "65/70 [==========================>...] - ETA: 0s - loss: 0.0826 - custom_accuracy: 0.8146{'loss': 0.0814293920993805, 'custom_accuracy': 0.8199999928474426}\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0814 - custom_accuracy: 0.8200\n",
            "345\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_16 (Bidirect  (None, 35, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_17 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "69/69 [==============================] - 6s 9ms/step - loss: 0.2247 - custom_accuracy: 0.3587\n",
            "Epoch 2/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1755 - custom_accuracy: 0.5130\n",
            "Epoch 3/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1717 - custom_accuracy: 0.5029\n",
            "Epoch 4/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1693 - custom_accuracy: 0.5399\n",
            "Epoch 5/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1586 - custom_accuracy: 0.5674\n",
            "Epoch 6/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1519 - custom_accuracy: 0.5783\n",
            "Epoch 7/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1481 - custom_accuracy: 0.6159\n",
            "Epoch 8/100\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.1396 - custom_accuracy: 0.6326\n",
            "Epoch 9/100\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.1323 - custom_accuracy: 0.6399\n",
            "Epoch 10/100\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.1270 - custom_accuracy: 0.6536\n",
            "Epoch 11/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1208 - custom_accuracy: 0.6696\n",
            "Epoch 12/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1221 - custom_accuracy: 0.6609\n",
            "Epoch 13/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1162 - custom_accuracy: 0.6580\n",
            "Epoch 14/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1155 - custom_accuracy: 0.6978\n",
            "Epoch 15/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1144 - custom_accuracy: 0.6935\n",
            "Epoch 16/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1115 - custom_accuracy: 0.6986\n",
            "Epoch 17/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1142 - custom_accuracy: 0.6906\n",
            "Epoch 18/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1110 - custom_accuracy: 0.7101\n",
            "Epoch 19/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1097 - custom_accuracy: 0.7080\n",
            "Epoch 20/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1103 - custom_accuracy: 0.6957\n",
            "Epoch 21/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1116 - custom_accuracy: 0.7123\n",
            "Epoch 22/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1080 - custom_accuracy: 0.7196\n",
            "Epoch 23/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1073 - custom_accuracy: 0.7370\n",
            "Epoch 24/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1090 - custom_accuracy: 0.7290\n",
            "Epoch 25/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1035 - custom_accuracy: 0.7362\n",
            "Epoch 26/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1065 - custom_accuracy: 0.7254\n",
            "Epoch 27/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1058 - custom_accuracy: 0.7188\n",
            "Epoch 28/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.1066 - custom_accuracy: 0.7239\n",
            "Epoch 29/100\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.1055 - custom_accuracy: 0.7312\n",
            "Epoch 30/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1041 - custom_accuracy: 0.7370\n",
            "Epoch 31/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1081 - custom_accuracy: 0.7326\n",
            "Epoch 32/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1026 - custom_accuracy: 0.7435\n",
            "Epoch 33/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1051 - custom_accuracy: 0.7406\n",
            "Epoch 34/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1059 - custom_accuracy: 0.7355\n",
            "Epoch 35/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1023 - custom_accuracy: 0.7536\n",
            "Epoch 36/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1013 - custom_accuracy: 0.7659\n",
            "Epoch 37/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1068 - custom_accuracy: 0.7391\n",
            "Epoch 38/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1011 - custom_accuracy: 0.7601\n",
            "Epoch 39/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1014 - custom_accuracy: 0.7522\n",
            "Epoch 40/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0990 - custom_accuracy: 0.7572\n",
            "Epoch 41/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1033 - custom_accuracy: 0.7413\n",
            "Epoch 42/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1018 - custom_accuracy: 0.7601\n",
            "Epoch 43/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0998 - custom_accuracy: 0.7652\n",
            "Epoch 44/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1005 - custom_accuracy: 0.7558\n",
            "Epoch 45/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1000 - custom_accuracy: 0.7652\n",
            "Epoch 46/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1027 - custom_accuracy: 0.7493\n",
            "Epoch 47/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1000 - custom_accuracy: 0.7638\n",
            "Epoch 48/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1010 - custom_accuracy: 0.7572\n",
            "Epoch 49/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0993 - custom_accuracy: 0.7536\n",
            "Epoch 50/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.1002 - custom_accuracy: 0.7616\n",
            "Epoch 51/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0980 - custom_accuracy: 0.7804\n",
            "Epoch 52/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0987 - custom_accuracy: 0.7710\n",
            "Epoch 53/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0958 - custom_accuracy: 0.7804\n",
            "Epoch 54/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0995 - custom_accuracy: 0.7609\n",
            "Epoch 55/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0993 - custom_accuracy: 0.7594\n",
            "Epoch 56/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0967 - custom_accuracy: 0.7775\n",
            "Epoch 57/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0955 - custom_accuracy: 0.7855\n",
            "Epoch 58/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0925 - custom_accuracy: 0.7899\n",
            "Epoch 59/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0950 - custom_accuracy: 0.7957\n",
            "Epoch 60/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0950 - custom_accuracy: 0.7826\n",
            "Epoch 61/100\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.0943 - custom_accuracy: 0.7833\n",
            "Epoch 62/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0947 - custom_accuracy: 0.7819\n",
            "Epoch 63/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0925 - custom_accuracy: 0.8043\n",
            "Epoch 64/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0934 - custom_accuracy: 0.7862\n",
            "Epoch 65/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0913 - custom_accuracy: 0.8014\n",
            "Epoch 66/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0968 - custom_accuracy: 0.7928\n",
            "Epoch 67/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0920 - custom_accuracy: 0.7906\n",
            "Epoch 68/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0880 - custom_accuracy: 0.8043\n",
            "Epoch 69/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0910 - custom_accuracy: 0.7964\n",
            "Epoch 70/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0896 - custom_accuracy: 0.8080\n",
            "Epoch 71/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0899 - custom_accuracy: 0.7920\n",
            "Epoch 72/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0888 - custom_accuracy: 0.8130\n",
            "Epoch 73/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0880 - custom_accuracy: 0.8109\n",
            "Epoch 74/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0877 - custom_accuracy: 0.8239\n",
            "Epoch 75/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0885 - custom_accuracy: 0.8152\n",
            "Epoch 76/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0877 - custom_accuracy: 0.8167\n",
            "Epoch 77/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0870 - custom_accuracy: 0.8268\n",
            "Epoch 78/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0854 - custom_accuracy: 0.8261\n",
            "Epoch 79/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0903 - custom_accuracy: 0.8058\n",
            "Epoch 80/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0848 - custom_accuracy: 0.8333\n",
            "Epoch 81/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0832 - custom_accuracy: 0.8319\n",
            "Epoch 82/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0869 - custom_accuracy: 0.8181\n",
            "Epoch 83/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0820 - custom_accuracy: 0.8348\n",
            "Epoch 84/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0838 - custom_accuracy: 0.8319\n",
            "Epoch 85/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0864 - custom_accuracy: 0.8159\n",
            "Epoch 86/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0841 - custom_accuracy: 0.8268\n",
            "Epoch 87/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0836 - custom_accuracy: 0.8428\n",
            "Epoch 88/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0832 - custom_accuracy: 0.8275\n",
            "Epoch 89/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0841 - custom_accuracy: 0.8268\n",
            "Epoch 90/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0813 - custom_accuracy: 0.8304\n",
            "Epoch 91/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0799 - custom_accuracy: 0.8283\n",
            "Epoch 92/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0816 - custom_accuracy: 0.8319\n",
            "Epoch 93/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0812 - custom_accuracy: 0.8326\n",
            "Epoch 94/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0806 - custom_accuracy: 0.8464\n",
            "Epoch 95/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0783 - custom_accuracy: 0.8399\n",
            "Epoch 96/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0806 - custom_accuracy: 0.8333\n",
            "Epoch 97/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0786 - custom_accuracy: 0.8413\n",
            "Epoch 98/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0784 - custom_accuracy: 0.8406\n",
            "Epoch 99/100\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0810 - custom_accuracy: 0.8514\n",
            "Epoch 100/100\n",
            "67/69 [============================>.] - ETA: 0s - loss: 0.0785 - custom_accuracy: 0.8343{'loss': 0.07863249629735947, 'custom_accuracy': 0.834782600402832}\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.0786 - custom_accuracy: 0.8348\n",
            "340\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_18 (Bidirect  (None, 40, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_19 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "68/68 [==============================] - 5s 9ms/step - loss: 0.2226 - custom_accuracy: 0.3772\n",
            "Epoch 2/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1880 - custom_accuracy: 0.4522\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1811 - custom_accuracy: 0.4831\n",
            "Epoch 4/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1756 - custom_accuracy: 0.5029\n",
            "Epoch 5/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1720 - custom_accuracy: 0.5228\n",
            "Epoch 6/100\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1681 - custom_accuracy: 0.5404\n",
            "Epoch 7/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1657 - custom_accuracy: 0.5522\n",
            "Epoch 8/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1591 - custom_accuracy: 0.5897\n",
            "Epoch 9/100\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1519 - custom_accuracy: 0.6037\n",
            "Epoch 10/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1468 - custom_accuracy: 0.6162\n",
            "Epoch 11/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1418 - custom_accuracy: 0.6463\n",
            "Epoch 12/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1372 - custom_accuracy: 0.6368\n",
            "Epoch 13/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1281 - custom_accuracy: 0.6846\n",
            "Epoch 14/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1260 - custom_accuracy: 0.6772\n",
            "Epoch 15/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1235 - custom_accuracy: 0.6904\n",
            "Epoch 16/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1189 - custom_accuracy: 0.7191\n",
            "Epoch 17/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1166 - custom_accuracy: 0.7088\n",
            "Epoch 18/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1176 - custom_accuracy: 0.7228\n",
            "Epoch 19/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1168 - custom_accuracy: 0.7110\n",
            "Epoch 20/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1180 - custom_accuracy: 0.7088\n",
            "Epoch 21/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1130 - custom_accuracy: 0.7412\n",
            "Epoch 22/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1121 - custom_accuracy: 0.7360\n",
            "Epoch 23/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1122 - custom_accuracy: 0.7390\n",
            "Epoch 24/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1163 - custom_accuracy: 0.7250\n",
            "Epoch 25/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1129 - custom_accuracy: 0.7434\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1122 - custom_accuracy: 0.7346\n",
            "Epoch 27/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1088 - custom_accuracy: 0.7515\n",
            "Epoch 28/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1114 - custom_accuracy: 0.7368\n",
            "Epoch 29/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1109 - custom_accuracy: 0.7375\n",
            "Epoch 30/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1089 - custom_accuracy: 0.7610\n",
            "Epoch 31/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1053 - custom_accuracy: 0.7559\n",
            "Epoch 32/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1073 - custom_accuracy: 0.7662\n",
            "Epoch 33/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1065 - custom_accuracy: 0.7559\n",
            "Epoch 34/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1091 - custom_accuracy: 0.7412\n",
            "Epoch 35/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1069 - custom_accuracy: 0.7493\n",
            "Epoch 36/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1036 - custom_accuracy: 0.7684\n",
            "Epoch 37/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1066 - custom_accuracy: 0.7500\n",
            "Epoch 38/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1070 - custom_accuracy: 0.7551\n",
            "Epoch 39/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1003 - custom_accuracy: 0.7706\n",
            "Epoch 40/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1048 - custom_accuracy: 0.7713\n",
            "Epoch 41/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1020 - custom_accuracy: 0.7610\n",
            "Epoch 42/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1012 - custom_accuracy: 0.7772\n",
            "Epoch 43/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1006 - custom_accuracy: 0.7647\n",
            "Epoch 44/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1040 - custom_accuracy: 0.7699\n",
            "Epoch 45/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1001 - custom_accuracy: 0.7809\n",
            "Epoch 46/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1018 - custom_accuracy: 0.7750\n",
            "Epoch 47/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0994 - custom_accuracy: 0.7706\n",
            "Epoch 48/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0996 - custom_accuracy: 0.7728\n",
            "Epoch 49/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0968 - custom_accuracy: 0.7794\n",
            "Epoch 50/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0959 - custom_accuracy: 0.7882\n",
            "Epoch 51/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0989 - custom_accuracy: 0.7860\n",
            "Epoch 52/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0954 - custom_accuracy: 0.7875\n",
            "Epoch 53/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0983 - custom_accuracy: 0.7838\n",
            "Epoch 54/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0962 - custom_accuracy: 0.7993\n",
            "Epoch 55/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0955 - custom_accuracy: 0.8059\n",
            "Epoch 56/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0942 - custom_accuracy: 0.8007\n",
            "Epoch 57/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0926 - custom_accuracy: 0.8184\n",
            "Epoch 58/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0926 - custom_accuracy: 0.8103\n",
            "Epoch 59/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0927 - custom_accuracy: 0.7941\n",
            "Epoch 60/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0912 - custom_accuracy: 0.8118\n",
            "Epoch 61/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0909 - custom_accuracy: 0.8213\n",
            "Epoch 62/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0923 - custom_accuracy: 0.8110\n",
            "Epoch 63/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0920 - custom_accuracy: 0.8147\n",
            "Epoch 64/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0924 - custom_accuracy: 0.8169\n",
            "Epoch 65/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0912 - custom_accuracy: 0.8287\n",
            "Epoch 66/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0889 - custom_accuracy: 0.8191\n",
            "Epoch 67/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0898 - custom_accuracy: 0.8228\n",
            "Epoch 68/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0868 - custom_accuracy: 0.8331\n",
            "Epoch 69/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0881 - custom_accuracy: 0.8316\n",
            "Epoch 70/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0889 - custom_accuracy: 0.8309\n",
            "Epoch 71/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0874 - custom_accuracy: 0.8250\n",
            "Epoch 72/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0873 - custom_accuracy: 0.8294\n",
            "Epoch 73/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0853 - custom_accuracy: 0.8331\n",
            "Epoch 74/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0864 - custom_accuracy: 0.8272\n",
            "Epoch 75/100\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.0845 - custom_accuracy: 0.8382\n",
            "Epoch 76/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0874 - custom_accuracy: 0.8243\n",
            "Epoch 77/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0882 - custom_accuracy: 0.8140\n",
            "Epoch 78/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0872 - custom_accuracy: 0.8279\n",
            "Epoch 79/100\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.0835 - custom_accuracy: 0.8279\n",
            "Epoch 80/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0840 - custom_accuracy: 0.8368\n",
            "Epoch 81/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0843 - custom_accuracy: 0.8375\n",
            "Epoch 82/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0844 - custom_accuracy: 0.8294\n",
            "Epoch 83/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0816 - custom_accuracy: 0.8404\n",
            "Epoch 84/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0847 - custom_accuracy: 0.8346\n",
            "Epoch 85/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0824 - custom_accuracy: 0.8353\n",
            "Epoch 86/100\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0799 - custom_accuracy: 0.8426\n",
            "Epoch 87/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0778 - custom_accuracy: 0.8419\n",
            "Epoch 88/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0801 - custom_accuracy: 0.8610\n",
            "Epoch 89/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0822 - custom_accuracy: 0.8404\n",
            "Epoch 90/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0791 - custom_accuracy: 0.8368\n",
            "Epoch 91/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0787 - custom_accuracy: 0.8478\n",
            "Epoch 92/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0780 - custom_accuracy: 0.8566\n",
            "Epoch 93/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0793 - custom_accuracy: 0.8456\n",
            "Epoch 94/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0790 - custom_accuracy: 0.8507\n",
            "Epoch 95/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0765 - custom_accuracy: 0.8522\n",
            "Epoch 96/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0798 - custom_accuracy: 0.8375\n",
            "Epoch 97/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0786 - custom_accuracy: 0.8426\n",
            "Epoch 98/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0778 - custom_accuracy: 0.8654\n",
            "Epoch 99/100\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0788 - custom_accuracy: 0.8419\n",
            "Epoch 100/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0772 - custom_accuracy: 0.8500{'loss': 0.07701584696769714, 'custom_accuracy': 0.8499999046325684}\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0770 - custom_accuracy: 0.8500\n",
            "335\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_20 (Bidirect  (None, 45, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_21 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "67/67 [==============================] - 6s 10ms/step - loss: 0.2380 - custom_accuracy: 0.3246\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1835 - custom_accuracy: 0.4246\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1746 - custom_accuracy: 0.5284\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1715 - custom_accuracy: 0.5321\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1643 - custom_accuracy: 0.5448\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1539 - custom_accuracy: 0.5888\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1492 - custom_accuracy: 0.6269\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1463 - custom_accuracy: 0.6254\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1355 - custom_accuracy: 0.6642\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1297 - custom_accuracy: 0.6746\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1301 - custom_accuracy: 0.6821\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1305 - custom_accuracy: 0.6761\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1215 - custom_accuracy: 0.7164\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1168 - custom_accuracy: 0.7224\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1186 - custom_accuracy: 0.7164\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1180 - custom_accuracy: 0.7269\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1147 - custom_accuracy: 0.7351\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1116 - custom_accuracy: 0.7381\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1113 - custom_accuracy: 0.7604\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1092 - custom_accuracy: 0.7507\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1094 - custom_accuracy: 0.7537\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1071 - custom_accuracy: 0.7530\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1046 - custom_accuracy: 0.7634\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1036 - custom_accuracy: 0.7612\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1044 - custom_accuracy: 0.7604\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1064 - custom_accuracy: 0.7328\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1055 - custom_accuracy: 0.7612\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1036 - custom_accuracy: 0.7739\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1055 - custom_accuracy: 0.7388\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1035 - custom_accuracy: 0.7672\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1015 - custom_accuracy: 0.7746\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1008 - custom_accuracy: 0.7687\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0999 - custom_accuracy: 0.7873\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1034 - custom_accuracy: 0.7672\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1032 - custom_accuracy: 0.7709\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0996 - custom_accuracy: 0.7836\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1009 - custom_accuracy: 0.7813\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0985 - custom_accuracy: 0.7828\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1011 - custom_accuracy: 0.7813\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0989 - custom_accuracy: 0.7881\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0989 - custom_accuracy: 0.7903\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0982 - custom_accuracy: 0.7993\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0978 - custom_accuracy: 0.7873\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0962 - custom_accuracy: 0.7910\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0953 - custom_accuracy: 0.7963\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0947 - custom_accuracy: 0.8037\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0957 - custom_accuracy: 0.7955\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0962 - custom_accuracy: 0.7940\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0931 - custom_accuracy: 0.8127\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0953 - custom_accuracy: 0.8082\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0934 - custom_accuracy: 0.8194\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0936 - custom_accuracy: 0.8157\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0940 - custom_accuracy: 0.8022\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0919 - custom_accuracy: 0.8142\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0910 - custom_accuracy: 0.8224\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0933 - custom_accuracy: 0.8216\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0909 - custom_accuracy: 0.8216\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0900 - custom_accuracy: 0.8291\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0890 - custom_accuracy: 0.8246\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0912 - custom_accuracy: 0.8239\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0904 - custom_accuracy: 0.8164\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0879 - custom_accuracy: 0.8358\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0903 - custom_accuracy: 0.8216\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0886 - custom_accuracy: 0.8291\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0895 - custom_accuracy: 0.8179\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0929 - custom_accuracy: 0.8216\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0886 - custom_accuracy: 0.8418\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0898 - custom_accuracy: 0.8276\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0881 - custom_accuracy: 0.8261\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0917 - custom_accuracy: 0.8254\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0868 - custom_accuracy: 0.8351\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0899 - custom_accuracy: 0.8351\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0845 - custom_accuracy: 0.8448\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0889 - custom_accuracy: 0.8448\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0879 - custom_accuracy: 0.8299\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0880 - custom_accuracy: 0.8425\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0890 - custom_accuracy: 0.8328\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0866 - custom_accuracy: 0.8410\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0860 - custom_accuracy: 0.8358\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0854 - custom_accuracy: 0.8448\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0853 - custom_accuracy: 0.8366\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0859 - custom_accuracy: 0.8366\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0840 - custom_accuracy: 0.8425\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0867 - custom_accuracy: 0.8373\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0833 - custom_accuracy: 0.8463\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0833 - custom_accuracy: 0.8470\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0829 - custom_accuracy: 0.8410\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0813 - custom_accuracy: 0.8530\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0845 - custom_accuracy: 0.8507\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0828 - custom_accuracy: 0.8463\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0822 - custom_accuracy: 0.8455\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0846 - custom_accuracy: 0.8418\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0821 - custom_accuracy: 0.8478\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0806 - custom_accuracy: 0.8440\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0819 - custom_accuracy: 0.8448\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0836 - custom_accuracy: 0.8485\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0832 - custom_accuracy: 0.8448\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0797 - custom_accuracy: 0.8575\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0798 - custom_accuracy: 0.8522\n",
            "Epoch 100/100\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0803 - custom_accuracy: 0.8545{'loss': 0.08121147751808167, 'custom_accuracy': 0.847014844417572}\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0812 - custom_accuracy: 0.8470\n",
            "330\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_22 (Bidirect  (None, 50, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_23 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 10ms/step - loss: 0.2272 - custom_accuracy: 0.3364\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1938 - custom_accuracy: 0.4114\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1869 - custom_accuracy: 0.4629\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1834 - custom_accuracy: 0.4811\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1725 - custom_accuracy: 0.5008\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1725 - custom_accuracy: 0.5182\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1553 - custom_accuracy: 0.6023\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1510 - custom_accuracy: 0.6212\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1438 - custom_accuracy: 0.6356\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1361 - custom_accuracy: 0.6553\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1322 - custom_accuracy: 0.6735\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1280 - custom_accuracy: 0.6947\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1265 - custom_accuracy: 0.6773\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1237 - custom_accuracy: 0.7250\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1209 - custom_accuracy: 0.7121\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1178 - custom_accuracy: 0.7129\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1165 - custom_accuracy: 0.7250\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1156 - custom_accuracy: 0.7515\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1168 - custom_accuracy: 0.7318\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1109 - custom_accuracy: 0.7659\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1154 - custom_accuracy: 0.7212\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.1137 - custom_accuracy: 0.7644\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1159 - custom_accuracy: 0.7477\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1110 - custom_accuracy: 0.7621\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1119 - custom_accuracy: 0.7568\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1119 - custom_accuracy: 0.7477\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1089 - custom_accuracy: 0.7705\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1104 - custom_accuracy: 0.7583\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1096 - custom_accuracy: 0.7788\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1056 - custom_accuracy: 0.7462\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1160 - custom_accuracy: 0.7447\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1071 - custom_accuracy: 0.7667\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1080 - custom_accuracy: 0.7705\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1032 - custom_accuracy: 0.7712\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1043 - custom_accuracy: 0.7674\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1060 - custom_accuracy: 0.7811\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1037 - custom_accuracy: 0.7894\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1043 - custom_accuracy: 0.7894\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1002 - custom_accuracy: 0.7909\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1033 - custom_accuracy: 0.7947\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1024 - custom_accuracy: 0.7833\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1025 - custom_accuracy: 0.7962\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1014 - custom_accuracy: 0.8045\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1011 - custom_accuracy: 0.7909\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1050 - custom_accuracy: 0.7758\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1035 - custom_accuracy: 0.7788\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1013 - custom_accuracy: 0.7939\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1005 - custom_accuracy: 0.8008\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.1009 - custom_accuracy: 0.8023\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1020 - custom_accuracy: 0.7962\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1003 - custom_accuracy: 0.8008\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.1005 - custom_accuracy: 0.7909\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0988 - custom_accuracy: 0.8045\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0988 - custom_accuracy: 0.7864\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0997 - custom_accuracy: 0.7962\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0988 - custom_accuracy: 0.8098\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0986 - custom_accuracy: 0.8023\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 1s 12ms/step - loss: 0.0982 - custom_accuracy: 0.8038\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0991 - custom_accuracy: 0.8023\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0980 - custom_accuracy: 0.8000\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0988 - custom_accuracy: 0.8045\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0966 - custom_accuracy: 0.8008\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0989 - custom_accuracy: 0.7992\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0952 - custom_accuracy: 0.8068\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0966 - custom_accuracy: 0.7886\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0976 - custom_accuracy: 0.8061\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0959 - custom_accuracy: 0.8098\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0974 - custom_accuracy: 0.8227\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0944 - custom_accuracy: 0.8144\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0898 - custom_accuracy: 0.8159\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0950 - custom_accuracy: 0.8114\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0971 - custom_accuracy: 0.8061\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0950 - custom_accuracy: 0.8129\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0940 - custom_accuracy: 0.8114\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0914 - custom_accuracy: 0.8242\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0936 - custom_accuracy: 0.8174\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0903 - custom_accuracy: 0.8258\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0925 - custom_accuracy: 0.8197\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0930 - custom_accuracy: 0.8152\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0911 - custom_accuracy: 0.8348\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0895 - custom_accuracy: 0.8333\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0903 - custom_accuracy: 0.8295\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0869 - custom_accuracy: 0.8402\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0926 - custom_accuracy: 0.8273\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0882 - custom_accuracy: 0.8288\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0883 - custom_accuracy: 0.8318\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0874 - custom_accuracy: 0.8439\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0834 - custom_accuracy: 0.8439\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0855 - custom_accuracy: 0.8538\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0847 - custom_accuracy: 0.8477\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0828 - custom_accuracy: 0.8576\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 1s 11ms/step - loss: 0.0820 - custom_accuracy: 0.8591\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0816 - custom_accuracy: 0.8561\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0819 - custom_accuracy: 0.8523\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0815 - custom_accuracy: 0.8485\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0813 - custom_accuracy: 0.8523\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0855 - custom_accuracy: 0.8409\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0814 - custom_accuracy: 0.8530\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0788 - custom_accuracy: 0.8621\n",
            "Epoch 100/100\n",
            "61/66 [==========================>...] - ETA: 0s - loss: 0.0814 - custom_accuracy: 0.8590{'loss': 0.07969923317432404, 'custom_accuracy': 0.8613635301589966}\n",
            "66/66 [==============================] - 1s 10ms/step - loss: 0.0797 - custom_accuracy: 0.8614\n",
            "325\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_24 (Bidirect  (None, 55, 20)            1200      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_25 (Bidirect  (None, 20)                2480      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 84        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3764 (14.70 KB)\n",
            "Trainable params: 3764 (14.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "65/65 [==============================] - 6s 11ms/step - loss: 0.2390 - custom_accuracy: 0.3677\n",
            "Epoch 2/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1929 - custom_accuracy: 0.4454\n",
            "Epoch 3/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1867 - custom_accuracy: 0.4477\n",
            "Epoch 4/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1784 - custom_accuracy: 0.5285\n",
            "Epoch 5/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1763 - custom_accuracy: 0.5262\n",
            "Epoch 6/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1691 - custom_accuracy: 0.5708\n",
            "Epoch 7/100\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.1630 - custom_accuracy: 0.5646\n",
            "Epoch 8/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1560 - custom_accuracy: 0.6100\n",
            "Epoch 9/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1426 - custom_accuracy: 0.6308\n",
            "Epoch 10/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1411 - custom_accuracy: 0.6554\n",
            "Epoch 11/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1327 - custom_accuracy: 0.6931\n",
            "Epoch 12/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1283 - custom_accuracy: 0.7000\n",
            "Epoch 13/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1239 - custom_accuracy: 0.7015\n",
            "Epoch 14/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1174 - custom_accuracy: 0.7308\n",
            "Epoch 15/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1203 - custom_accuracy: 0.7000\n",
            "Epoch 16/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1164 - custom_accuracy: 0.7331\n",
            "Epoch 17/100\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.1176 - custom_accuracy: 0.7385\n",
            "Epoch 18/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1171 - custom_accuracy: 0.7362\n",
            "Epoch 19/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1175 - custom_accuracy: 0.7346\n",
            "Epoch 20/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1147 - custom_accuracy: 0.7392\n",
            "Epoch 21/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1098 - custom_accuracy: 0.7569\n",
            "Epoch 22/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1155 - custom_accuracy: 0.7400\n",
            "Epoch 23/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1095 - custom_accuracy: 0.7477\n",
            "Epoch 24/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1119 - custom_accuracy: 0.7477\n",
            "Epoch 25/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1112 - custom_accuracy: 0.7508\n",
            "Epoch 26/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1072 - custom_accuracy: 0.7592\n",
            "Epoch 27/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1114 - custom_accuracy: 0.7546\n",
            "Epoch 28/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1113 - custom_accuracy: 0.7654\n",
            "Epoch 29/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1066 - custom_accuracy: 0.7692\n",
            "Epoch 30/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1099 - custom_accuracy: 0.7554\n",
            "Epoch 31/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1076 - custom_accuracy: 0.7623\n",
            "Epoch 32/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1053 - custom_accuracy: 0.7638\n",
            "Epoch 33/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1062 - custom_accuracy: 0.7623\n",
            "Epoch 34/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1081 - custom_accuracy: 0.7754\n",
            "Epoch 35/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1045 - custom_accuracy: 0.7569\n",
            "Epoch 36/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1043 - custom_accuracy: 0.7769\n",
            "Epoch 37/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1047 - custom_accuracy: 0.7700\n",
            "Epoch 38/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.1007 - custom_accuracy: 0.7762\n",
            "Epoch 39/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0996 - custom_accuracy: 0.7838\n",
            "Epoch 40/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1039 - custom_accuracy: 0.7808\n",
            "Epoch 41/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1000 - custom_accuracy: 0.7808\n",
            "Epoch 42/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1001 - custom_accuracy: 0.7831\n",
            "Epoch 43/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1000 - custom_accuracy: 0.7831\n",
            "Epoch 44/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.1013 - custom_accuracy: 0.7800\n",
            "Epoch 45/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0969 - custom_accuracy: 0.7908\n",
            "Epoch 46/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0986 - custom_accuracy: 0.7908\n",
            "Epoch 47/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0968 - custom_accuracy: 0.8015\n",
            "Epoch 48/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0979 - custom_accuracy: 0.8015\n",
            "Epoch 49/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0936 - custom_accuracy: 0.7985\n",
            "Epoch 50/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0955 - custom_accuracy: 0.8054\n",
            "Epoch 51/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0943 - custom_accuracy: 0.8138\n",
            "Epoch 52/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0939 - custom_accuracy: 0.8054\n",
            "Epoch 53/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0922 - custom_accuracy: 0.8123\n",
            "Epoch 54/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0931 - custom_accuracy: 0.8169\n",
            "Epoch 55/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0940 - custom_accuracy: 0.8069\n",
            "Epoch 56/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0939 - custom_accuracy: 0.8069\n",
            "Epoch 57/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0914 - custom_accuracy: 0.8046\n",
            "Epoch 58/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0948 - custom_accuracy: 0.8231\n",
            "Epoch 59/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0932 - custom_accuracy: 0.8146\n",
            "Epoch 60/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0893 - custom_accuracy: 0.8262\n",
            "Epoch 61/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0921 - custom_accuracy: 0.8077\n",
            "Epoch 62/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0907 - custom_accuracy: 0.8246\n",
            "Epoch 63/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0887 - custom_accuracy: 0.8246\n",
            "Epoch 64/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0890 - custom_accuracy: 0.8223\n",
            "Epoch 65/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0917 - custom_accuracy: 0.8146\n",
            "Epoch 66/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0905 - custom_accuracy: 0.8200\n",
            "Epoch 67/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0918 - custom_accuracy: 0.8185\n",
            "Epoch 68/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0918 - custom_accuracy: 0.8192\n",
            "Epoch 69/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0881 - custom_accuracy: 0.8238\n",
            "Epoch 70/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0913 - custom_accuracy: 0.8054\n",
            "Epoch 71/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0885 - custom_accuracy: 0.8262\n",
            "Epoch 72/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0890 - custom_accuracy: 0.8269\n",
            "Epoch 73/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0876 - custom_accuracy: 0.8285\n",
            "Epoch 74/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0880 - custom_accuracy: 0.8362\n",
            "Epoch 75/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0885 - custom_accuracy: 0.8254\n",
            "Epoch 76/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0913 - custom_accuracy: 0.8231\n",
            "Epoch 77/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0869 - custom_accuracy: 0.8254\n",
            "Epoch 78/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0843 - custom_accuracy: 0.8362\n",
            "Epoch 79/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0853 - custom_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0868 - custom_accuracy: 0.8323\n",
            "Epoch 81/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0866 - custom_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0833 - custom_accuracy: 0.8331\n",
            "Epoch 83/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0869 - custom_accuracy: 0.8308\n",
            "Epoch 84/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0843 - custom_accuracy: 0.8485\n",
            "Epoch 85/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0837 - custom_accuracy: 0.8269\n",
            "Epoch 86/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0872 - custom_accuracy: 0.8215\n",
            "Epoch 87/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0877 - custom_accuracy: 0.8346\n",
            "Epoch 88/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0852 - custom_accuracy: 0.8385\n",
            "Epoch 89/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0852 - custom_accuracy: 0.8269\n",
            "Epoch 90/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0817 - custom_accuracy: 0.8438\n",
            "Epoch 91/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0819 - custom_accuracy: 0.8500\n",
            "Epoch 92/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0806 - custom_accuracy: 0.8523\n",
            "Epoch 93/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0838 - custom_accuracy: 0.8408\n",
            "Epoch 94/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0813 - custom_accuracy: 0.8469\n",
            "Epoch 95/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0816 - custom_accuracy: 0.8523\n",
            "Epoch 96/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0789 - custom_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0792 - custom_accuracy: 0.8538\n",
            "Epoch 98/100\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0770 - custom_accuracy: 0.8569\n",
            "Epoch 99/100\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0826 - custom_accuracy: 0.8408\n",
            "Epoch 100/100\n",
            "61/65 [===========================>..] - ETA: 0s - loss: 0.0762 - custom_accuracy: 0.8590{'loss': 0.07672284543514252, 'custom_accuracy': 0.8553845286369324}\n",
            "65/65 [==============================] - 1s 11ms/step - loss: 0.0767 - custom_accuracy: 0.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PrSWton5vFvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i in range(len(models[11:])):\n",
        "  X_test = last_trains[i]\n",
        "  Y_pred = []\n",
        "  for j in range(len(df_test)):\n",
        "    pred = models[i].predict(X_test, verbose = 0)\n",
        "    pred = scaler.inverse_transform(pred)\n",
        "    Y_pred.append(np.squeeze(np.array(pred), axis = 0))\n",
        "    X_test = X_test[0][1:]\n",
        "    pred = scaler.transform(pred)\n",
        "    X_test = np.append(X_test, pred, axis = 0)\n",
        "    X_test = np.expand_dims(X_test, axis=0)\n",
        "  predictions.append(Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG1Z3XqlfUY4",
        "outputId": "ecb62d15-6daf-45dc-837d-69e25166daf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKepVsh304MD",
        "outputId": "6d952c1c-7bfa-42b9-d5bf-df835dae7ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([45.322998, 45.121094, 47.06077 , 43.566727], dtype=float32),\n",
              "  array([45.606724, 45.374565, 47.37713 , 43.810223], dtype=float32),\n",
              "  array([45.85009 , 45.615025, 47.627956, 44.04226 ], dtype=float32),\n",
              "  array([46.068794, 45.8313  , 47.852325, 44.252094], dtype=float32),\n",
              "  array([46.265686, 46.026024, 48.054302, 44.44099 ], dtype=float32),\n",
              "  array([46.443096, 46.201504, 48.23629 , 44.611176], dtype=float32),\n",
              "  array([46.60307 , 46.359753, 48.4004  , 44.764618], dtype=float32),\n",
              "  array([46.747425, 46.502563, 48.54848 , 44.903065], dtype=float32),\n",
              "  array([46.877766, 46.63152 , 48.682194, 45.02806 ], dtype=float32),\n",
              "  array([46.99552 , 46.748028, 48.802994, 45.140976], dtype=float32),\n",
              "  array([47.101948, 46.853348, 48.912178, 45.243027], dtype=float32),\n",
              "  array([47.198193, 46.948586, 49.01091 , 45.335304], dtype=float32),\n",
              "  array([47.28525 , 47.034744, 49.100227, 45.41877 ], dtype=float32),\n",
              "  array([47.36404 , 47.112713, 49.181053, 45.494305], dtype=float32),\n",
              "  array([47.43536 , 47.183304, 49.25422 , 45.56267 ], dtype=float32),\n",
              "  array([47.49994 , 47.247223, 49.32047 , 45.624577], dtype=float32),\n",
              "  array([47.558434, 47.30512 , 49.380478, 45.680645], dtype=float32),\n",
              "  array([47.611423, 47.35757 , 49.43484 , 45.73144 ], dtype=float32),\n",
              "  array([47.65944 , 47.4051  , 49.484108, 45.777462], dtype=float32),\n",
              "  array([47.70296 , 47.448177, 49.52875 , 45.81917 ], dtype=float32),\n",
              "  array([47.74241 , 47.48723 , 49.569225, 45.85698 ], dtype=float32),\n",
              "  array([47.77817 , 47.522633, 49.605915, 45.891262], dtype=float32),\n",
              "  array([47.8106  , 47.554733, 49.639187, 45.92234 ], dtype=float32),\n",
              "  array([47.840008, 47.583843, 49.669353, 45.95052 ], dtype=float32),\n",
              "  array([47.86668 , 47.61025 , 49.696716, 45.97608 ], dtype=float32),\n",
              "  array([47.890873, 47.6342  , 49.721535, 45.999268], dtype=float32),\n",
              "  array([47.91282 , 47.655926, 49.744053, 46.020298], dtype=float32),\n",
              "  array([47.932728, 47.67564 , 49.764477, 46.039375], dtype=float32),\n",
              "  array([47.95079 , 47.693523, 49.78301 , 46.056686], dtype=float32),\n",
              "  array([47.967182, 47.709747, 49.799824, 46.072395], dtype=float32),\n",
              "  array([47.98205 , 47.724476, 49.815086, 46.086647], dtype=float32),\n",
              "  array([47.995552, 47.73784 , 49.828934, 46.099583], dtype=float32),\n",
              "  array([48.007797, 47.74996 , 49.8415  , 46.111317], dtype=float32),\n",
              "  array([48.018913, 47.760967, 49.8529  , 46.12197 ], dtype=float32),\n",
              "  array([48.029   , 47.77095 , 49.86325 , 46.131634], dtype=float32),\n",
              "  array([48.038155, 47.780018, 49.872643, 46.140408], dtype=float32),\n",
              "  array([48.04647 , 47.788246, 49.88117 , 46.148373], dtype=float32),\n",
              "  array([48.054012, 47.795715, 49.888912, 46.155605], dtype=float32),\n",
              "  array([48.06086 , 47.802494, 49.895935, 46.162167], dtype=float32),\n",
              "  array([48.067078, 47.808655, 49.90231 , 46.16812 ], dtype=float32),\n",
              "  array([48.07272 , 47.81424 , 49.908104, 46.17353 ], dtype=float32),\n",
              "  array([48.077843, 47.81931 , 49.913357, 46.178436], dtype=float32),\n",
              "  array([48.082493, 47.823914, 49.91813 , 46.18289 ], dtype=float32),\n",
              "  array([48.08671 , 47.828094, 49.92246 , 46.18694 ], dtype=float32),\n",
              "  array([48.090546, 47.831882, 49.92639 , 46.19061 ], dtype=float32),\n",
              "  array([48.09402, 47.83533, 49.92996, 46.19394], dtype=float32),\n",
              "  array([48.097176, 47.83846 , 49.933193, 46.196964], dtype=float32),\n",
              "  array([48.100044, 47.841293, 49.93614 , 46.199715], dtype=float32),\n",
              "  array([48.102646, 47.84387 , 49.93881 , 46.202206], dtype=float32),\n",
              "  array([48.10501 , 47.846207, 49.94123 , 46.204468], dtype=float32),\n",
              "  array([48.107155, 47.84833 , 49.94343 , 46.206528], dtype=float32),\n",
              "  array([48.109104, 47.85026 , 49.94543 , 46.208393], dtype=float32),\n",
              "  array([48.110867, 47.85201 , 49.947247, 46.210083], dtype=float32),\n",
              "  array([48.112473, 47.8536  , 49.94889 , 46.21162 ], dtype=float32),\n",
              "  array([48.11393 , 47.85504 , 49.950382, 46.21302 ], dtype=float32),\n",
              "  array([48.115253, 47.856354, 49.951744, 46.214287], dtype=float32),\n",
              "  array([48.116455, 47.85754 , 49.952976, 46.21544 ], dtype=float32),\n",
              "  array([48.11755 , 47.858624, 49.954094, 46.216488], dtype=float32),\n",
              "  array([48.11854 , 47.859608, 49.955112, 46.217438], dtype=float32),\n",
              "  array([48.119442, 47.860497, 49.956036, 46.2183  ], dtype=float32),\n",
              "  array([48.12026 , 47.861305, 49.956875, 46.21908 ], dtype=float32),\n",
              "  array([48.121002, 47.86204 , 49.957638, 46.219795], dtype=float32),\n",
              "  array([48.121677, 47.86271 , 49.95833 , 46.22044 ], dtype=float32),\n",
              "  array([48.122284, 47.86331 , 49.958958, 46.221024], dtype=float32),\n",
              "  array([48.12284 , 47.863865, 49.959522, 46.221558], dtype=float32),\n",
              "  array([48.123344, 47.864365, 49.96004 , 46.22204 ], dtype=float32),\n",
              "  array([48.123802, 47.86482 , 49.960514, 46.222477], dtype=float32),\n",
              "  array([48.12422 , 47.865227, 49.96094 , 46.222878], dtype=float32),\n",
              "  array([48.124596, 47.865604, 49.961327, 46.22324 ], dtype=float32),\n",
              "  array([48.12494 , 47.865944, 49.961678, 46.223568], dtype=float32),\n",
              "  array([48.12525 , 47.86625 , 49.961998, 46.223866], dtype=float32),\n",
              "  array([48.125534, 47.86653 , 49.962288, 46.224136], dtype=float32),\n",
              "  array([48.12579 , 47.866783, 49.962547, 46.22438 ], dtype=float32),\n",
              "  array([48.126022, 47.867012, 49.962788, 46.2246  ], dtype=float32),\n",
              "  array([48.126232, 47.86722 , 49.963005, 46.224804], dtype=float32),\n",
              "  array([48.126427, 47.867413, 49.963203, 46.22499 ], dtype=float32),\n",
              "  array([48.126595, 47.867584, 49.963383, 46.225155], dtype=float32),\n",
              "  array([48.126755, 47.86774 , 49.963543, 46.225307], dtype=float32),\n",
              "  array([48.126896, 47.86788 , 49.963688, 46.22544 ], dtype=float32),\n",
              "  array([48.12703 , 47.86801 , 49.96382 , 46.225567], dtype=float32),\n",
              "  array([48.127144, 47.868126, 49.963943, 46.22568 ], dtype=float32),\n",
              "  array([48.127254, 47.868233, 49.964054, 46.225784], dtype=float32),\n",
              "  array([48.12735 , 47.868332, 49.964153, 46.22588 ], dtype=float32),\n",
              "  array([48.12744 , 47.86842 , 49.964245, 46.225964], dtype=float32),\n",
              "  array([48.12752 , 47.8685  , 49.96433 , 46.226044], dtype=float32),\n",
              "  array([48.127598, 47.868572, 49.9644  , 46.226116], dtype=float32),\n",
              "  array([48.127663, 47.868637, 49.964474, 46.226177], dtype=float32),\n",
              "  array([48.12772 , 47.8687  , 49.964535, 46.226234], dtype=float32),\n",
              "  array([48.127777, 47.86875 , 49.96459 , 46.226288], dtype=float32),\n",
              "  array([48.127827, 47.868797, 49.96464 , 46.226334], dtype=float32),\n",
              "  array([48.127872, 47.868847, 49.964687, 46.22638 ], dtype=float32),\n",
              "  array([48.127914, 47.86889 , 49.96473 , 46.22642 ], dtype=float32),\n",
              "  array([48.127953, 47.868923, 49.96477 , 46.226456], dtype=float32),\n",
              "  array([48.127987, 47.86896 , 49.964806, 46.226486], dtype=float32),\n",
              "  array([48.128017, 47.86899 , 49.964836, 46.22652 ], dtype=float32),\n",
              "  array([48.128044, 47.86902 , 49.964867, 46.226543], dtype=float32),\n",
              "  array([48.12807 , 47.86904 , 49.96489 , 46.226566], dtype=float32),\n",
              "  array([48.128094, 47.869064, 49.964912, 46.22659 ], dtype=float32),\n",
              "  array([48.128113, 47.869083, 49.964935, 46.22661 ], dtype=float32),\n",
              "  array([48.12813 , 47.869102, 49.96495 , 46.226624], dtype=float32),\n",
              "  array([48.128147, 47.869118, 49.96497 , 46.226643], dtype=float32),\n",
              "  array([48.128162, 47.869137, 49.964985, 46.226658], dtype=float32),\n",
              "  array([48.128178, 47.86915 , 49.965004, 46.22667 ], dtype=float32),\n",
              "  array([48.128193, 47.869164, 49.965015, 46.226685], dtype=float32),\n",
              "  array([48.128204, 47.869175, 49.96503 , 46.226696], dtype=float32),\n",
              "  array([48.128216, 47.869186, 49.96504 , 46.226707], dtype=float32),\n",
              "  array([48.128223, 47.869194, 49.965046, 46.22672 ], dtype=float32),\n",
              "  array([48.128235, 47.8692  , 49.965057, 46.226727], dtype=float32),\n",
              "  array([48.128242, 47.869213, 49.965065, 46.22673 ], dtype=float32),\n",
              "  array([48.12825 , 47.86922 , 49.965073, 46.226738], dtype=float32),\n",
              "  array([48.128258, 47.86923 , 49.965084, 46.22675 ], dtype=float32),\n",
              "  array([48.12826 , 47.869232, 49.96509 , 46.226753], dtype=float32),\n",
              "  array([48.128265, 47.869236, 49.96509 , 46.226757], dtype=float32),\n",
              "  array([48.12827 , 47.869236, 49.965096, 46.22676 ], dtype=float32),\n",
              "  array([48.128273, 47.86924 , 49.965103, 46.22676 ], dtype=float32),\n",
              "  array([48.128277, 47.869247, 49.965103, 46.226765], dtype=float32),\n",
              "  array([48.12828 , 47.86925 , 49.965107, 46.22677 ], dtype=float32),\n",
              "  array([48.128284, 47.869255, 49.96511 , 46.226772], dtype=float32),\n",
              "  array([48.12829 , 47.869255, 49.965115, 46.226776], dtype=float32),\n",
              "  array([48.128292, 47.86926 , 49.965115, 46.226776], dtype=float32),\n",
              "  array([48.128292, 47.86926 , 49.96512 , 46.22678 ], dtype=float32),\n",
              "  array([48.128296, 47.869263, 49.96512 , 46.22678 ], dtype=float32),\n",
              "  array([48.128296, 47.869263, 49.965122, 46.226784], dtype=float32),\n",
              "  array([48.128296, 47.869267, 49.965122, 46.226784], dtype=float32),\n",
              "  array([48.128296, 47.869267, 49.965126, 46.226784], dtype=float32),\n",
              "  array([48.1283  , 47.86927 , 49.965126, 46.226788], dtype=float32),\n",
              "  array([48.128304, 47.86927 , 49.965126, 46.22679 ], dtype=float32),\n",
              "  array([48.128304, 47.869274, 49.96513 , 46.22679 ], dtype=float32),\n",
              "  array([48.128304, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32),\n",
              "  array([48.128307, 47.869278, 49.965134, 46.22679 ], dtype=float32)],\n",
              " [array([47.314987, 46.919857, 48.63138 , 45.533646], dtype=float32),\n",
              "  array([48.888893, 48.463844, 50.294598, 47.105488], dtype=float32),\n",
              "  array([50.46101 , 50.06384 , 51.9078  , 48.701588], dtype=float32),\n",
              "  array([52.05083 , 51.63451 , 53.500916, 50.224567], dtype=float32),\n",
              "  array([53.605404, 53.176086, 55.05893 , 51.710796], dtype=float32),\n",
              "  array([55.12074 , 54.682274, 56.57741 , 53.15453 ], dtype=float32),\n",
              "  array([56.591343, 56.147343, 58.051052, 54.551777], dtype=float32),\n",
              "  array([58.011818, 57.56542 , 59.47457 , 55.898197], dtype=float32),\n",
              "  array([59.376934, 58.930805, 60.84281 , 57.18957 ], dtype=float32),\n",
              "  array([60.681828, 60.238163, 62.150974, 58.421963], dtype=float32),\n",
              "  array([61.9222  , 61.48273 , 63.394802, 59.59188 ], dtype=float32),\n",
              "  array([63.094486, 62.660496, 64.57071 , 60.696426], dtype=float32),\n",
              "  array([64.195946, 63.768333, 65.67595 , 61.73341 ], dtype=float32),\n",
              "  array([65.22476 , 64.804085, 66.708664, 62.701447], dtype=float32),\n",
              "  array([66.18008, 65.76661, 67.66797, 63.59997], dtype=float32),\n",
              "  array([67.06203 , 66.65577 , 68.55392 , 64.429276], dtype=float32),\n",
              "  array([67.87161, 67.47241, 69.36748, 65.19043], dtype=float32),\n",
              "  array([68.61069 , 68.218254, 70.110466, 65.885284], dtype=float32),\n",
              "  array([69.28186 , 68.89578 , 70.785416, 66.5163  ], dtype=float32),\n",
              "  array([69.88828, 69.50814, 71.39546, 67.0865 ], dtype=float32),\n",
              "  array([70.43363 , 70.05892 , 71.94424 , 67.599335], dtype=float32),\n",
              "  array([70.92187 , 70.55211 , 72.435684, 68.05853 ], dtype=float32),\n",
              "  array([71.35718, 70.99188, 72.87399, 68.46802], dtype=float32),\n",
              "  array([71.74383 , 71.38252 , 73.263374, 68.831795], dtype=float32),\n",
              "  array([72.086044, 71.72831 , 73.60811 , 69.15381 ], dtype=float32),\n",
              "  array([72.38798, 72.0334 , 73.91232, 69.43798], dtype=float32),\n",
              "  array([72.6536 , 72.30182, 74.17999, 69.68801], dtype=float32),\n",
              "  array([72.88668 , 72.53735 , 74.414925, 69.90745 ], dtype=float32),\n",
              "  array([73.09073 , 72.74355 , 74.620605, 70.09957 ], dtype=float32),\n",
              "  array([73.26899, 72.92368, 74.80032, 70.26743], dtype=float32),\n",
              "  array([73.42443 , 73.08078 , 74.957054, 70.413826], dtype=float32),\n",
              "  array([73.55975, 73.21753, 75.09352, 70.54129], dtype=float32),\n",
              "  array([73.6774  , 73.33642 , 75.212166, 70.65211 ], dtype=float32),\n",
              "  array([73.77954 , 73.439644, 75.315186, 70.74834 ], dtype=float32),\n",
              "  array([73.868126, 73.52917 , 75.40454 , 70.83179 ], dtype=float32),\n",
              "  array([73.94488 , 73.606735, 75.48196 , 70.904106], dtype=float32),\n",
              "  array([74.01132, 73.67388, 75.54899, 70.96671], dtype=float32),\n",
              "  array([74.0688  , 73.73197 , 75.60697 , 71.020874], dtype=float32),\n",
              "  array([74.11849 , 73.78218 , 75.657104, 71.067696], dtype=float32),\n",
              "  array([74.16142 , 73.82557 , 75.70042 , 71.108154], dtype=float32),\n",
              "  array([74.1985  , 73.863045, 75.73782 , 71.1431  ], dtype=float32),\n",
              "  array([74.23051 , 73.895386, 75.77012 , 71.17326 ], dtype=float32),\n",
              "  array([74.258125, 73.9233  , 75.79798 , 71.19929 ], dtype=float32),\n",
              "  array([74.28196, 73.94738, 75.82203, 71.22174], dtype=float32),\n",
              "  array([74.3025  , 73.96814 , 75.84276 , 71.241104], dtype=float32),\n",
              "  array([74.32021 , 73.986046, 75.860634, 71.257805], dtype=float32),\n",
              "  array([74.33548 , 74.00147 , 75.876045, 71.2722  ], dtype=float32),\n",
              "  array([74.34865, 74.01478, 75.88932, 71.2846 ], dtype=float32),\n",
              "  array([74.35999 , 74.02624 , 75.900764, 71.29529 ], dtype=float32),\n",
              "  array([74.36976 , 74.03611 , 75.91062 , 71.304504], dtype=float32),\n",
              "  array([74.37817, 74.04462, 75.91912, 71.31243], dtype=float32),\n",
              "  array([74.38543, 74.05194, 75.92644, 71.31927], dtype=float32),\n",
              "  array([74.39168 , 74.058266, 75.93274 , 71.325165], dtype=float32),\n",
              "  array([74.397064, 74.0637  , 75.93818 , 71.33024 ], dtype=float32),\n",
              "  array([74.401695, 74.06838 , 75.942856, 71.33461 ], dtype=float32),\n",
              "  array([74.40569 , 74.072426, 75.946884, 71.33837 ], dtype=float32),\n",
              "  array([74.409134, 74.0759  , 75.950356, 71.34161 ], dtype=float32),\n",
              "  array([74.41209 , 74.07889 , 75.95334 , 71.344406], dtype=float32),\n",
              "  array([74.41464, 74.08147, 75.95591, 71.34681], dtype=float32),\n",
              "  array([74.41684, 74.08369, 75.95813, 71.34888], dtype=float32),\n",
              "  array([74.41873 , 74.085594, 75.960045, 71.350655], dtype=float32),\n",
              "  array([74.42036 , 74.08724 , 75.961685, 71.352196], dtype=float32),\n",
              "  array([74.42175 , 74.08865 , 75.9631  , 71.353516], dtype=float32),\n",
              "  array([74.422966, 74.08988 , 75.96432 , 71.35465 ], dtype=float32),\n",
              "  array([74.424  , 74.09093, 75.96536, 71.35563], dtype=float32),\n",
              "  array([74.4249  , 74.09183 , 75.96626 , 71.356476], dtype=float32),\n",
              "  array([74.42567 , 74.092606, 75.96703 , 71.35719 ], dtype=float32),\n",
              "  array([74.42633, 74.09328, 75.96771, 71.35782], dtype=float32),\n",
              "  array([74.4269  , 74.09386 , 75.968285, 71.35836 ], dtype=float32),\n",
              "  array([74.4274  , 74.09435 , 75.96878 , 71.358826], dtype=float32),\n",
              "  array([74.42781 , 74.09478 , 75.969215, 71.35922 ], dtype=float32),\n",
              "  array([74.428185, 74.095146, 75.96958 , 71.359566], dtype=float32),\n",
              "  array([74.4285  , 74.09547 , 75.969894, 71.35987 ], dtype=float32),\n",
              "  array([74.42877, 74.09574, 75.97017, 71.36012], dtype=float32),\n",
              "  array([74.429   , 74.09598 , 75.970406, 71.36034 ], dtype=float32),\n",
              "  array([74.4292  , 74.096176, 75.970604, 71.36053 ], dtype=float32),\n",
              "  array([74.429375, 74.09635 , 75.97078 , 71.36069 ], dtype=float32),\n",
              "  array([74.42952 , 74.0965  , 75.970924, 71.360825], dtype=float32),\n",
              "  array([74.42965, 74.09662, 75.97105, 71.36095], dtype=float32),\n",
              "  array([74.42975 , 74.09673 , 75.97116 , 71.361046], dtype=float32),\n",
              "  array([74.42985 , 74.096825, 75.97126 , 71.36114 ], dtype=float32),\n",
              "  array([74.42993, 74.09691, 75.97134, 71.36121], dtype=float32),\n",
              "  array([74.42999 , 74.09698 , 75.971405, 71.361275], dtype=float32),\n",
              "  array([74.43006 , 74.097046, 75.971466, 71.36133 ], dtype=float32),\n",
              "  array([74.43011, 74.09709, 75.97152, 71.36138], dtype=float32),\n",
              "  array([74.43015 , 74.09714 , 75.971565, 71.36143 ], dtype=float32),\n",
              "  array([74.43018 , 74.097176, 75.9716  , 71.361465], dtype=float32),\n",
              "  array([74.43022 , 74.09721 , 75.97164 , 71.361496], dtype=float32),\n",
              "  array([74.43026 , 74.097244, 75.97167 , 71.36152 ], dtype=float32),\n",
              "  array([74.430275, 74.09727 , 75.9717  , 71.36155 ], dtype=float32),\n",
              "  array([74.4303  , 74.09729 , 75.97171 , 71.361565], dtype=float32),\n",
              "  array([74.43032, 74.09731, 75.97173, 71.36159], dtype=float32),\n",
              "  array([74.430336, 74.09733 , 75.97175 , 71.3616  ], dtype=float32),\n",
              "  array([74.43035, 74.09734, 75.97176, 71.36161], dtype=float32),\n",
              "  array([74.43036 , 74.09735 , 75.97178 , 71.361626], dtype=float32),\n",
              "  array([74.43037 , 74.09736 , 75.971794, 71.36163 ], dtype=float32),\n",
              "  array([74.43038 , 74.097374, 75.9718  , 71.36164 ], dtype=float32),\n",
              "  array([74.43039, 74.09738, 75.97181, 71.36165], dtype=float32),\n",
              "  array([74.4304 , 74.09739, 75.97182, 71.36166], dtype=float32),\n",
              "  array([74.4304  , 74.09739 , 75.97182 , 71.361664], dtype=float32),\n",
              "  array([74.4304  , 74.0974  , 75.971825, 71.36167 ], dtype=float32),\n",
              "  array([74.43041 , 74.097404, 75.97183 , 71.36167 ], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32),\n",
              "  array([74.43042, 74.09741, 75.97184, 71.36168], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lala = df_test.copy()\n",
        "lala[['Open', 'Close', 'High', 'Low']] = predictions[0]"
      ],
      "metadata": {
        "id": "-N_xLRf7nVXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lala.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Wddc3U3xo5gn",
        "outputId": "9dae320a-d414-48eb-8816-778e845bd646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date       Open      Close       High        Low\n",
              "0  2019-11-27  45.322998  45.121094  47.060768  43.566727\n",
              "1  2019-11-28  45.606724  45.374565  47.377129  43.810223\n",
              "2  2019-11-29  45.850090  45.615025  47.627956  44.042259\n",
              "3  2019-11-30  46.068794  45.831299  47.852325  44.252094\n",
              "4  2019-12-01  46.265686  46.026024  48.054302  44.440990"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2cfb30b-cbea-462f-a3ea-721043cfeb9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-11-27</td>\n",
              "      <td>45.322998</td>\n",
              "      <td>45.121094</td>\n",
              "      <td>47.060768</td>\n",
              "      <td>43.566727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-11-28</td>\n",
              "      <td>45.606724</td>\n",
              "      <td>45.374565</td>\n",
              "      <td>47.377129</td>\n",
              "      <td>43.810223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-11-29</td>\n",
              "      <td>45.850090</td>\n",
              "      <td>45.615025</td>\n",
              "      <td>47.627956</td>\n",
              "      <td>44.042259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-11-30</td>\n",
              "      <td>46.068794</td>\n",
              "      <td>45.831299</td>\n",
              "      <td>47.852325</td>\n",
              "      <td>44.252094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-12-01</td>\n",
              "      <td>46.265686</td>\n",
              "      <td>46.026024</td>\n",
              "      <td>48.054302</td>\n",
              "      <td>44.440990</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2cfb30b-cbea-462f-a3ea-721043cfeb9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2cfb30b-cbea-462f-a3ea-721043cfeb9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2cfb30b-cbea-462f-a3ea-721043cfeb9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1ad702cd-fa6d-4244-af45-e14d5d1706e0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ad702cd-fa6d-4244-af45-e14d5d1706e0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1ad702cd-fa6d-4244-af45-e14d5d1706e0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lala.to_csv(\"submission2.csv\", index=False)"
      ],
      "metadata": {
        "id": "mhze8I2npzkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c praktikum-2-rnn-if4074-2023 -f submission2.csv -m \"test 4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBXhZndcqBOE",
        "outputId": "8ff86e94-ff2e-43b0-bb4e-31f58d576a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 8.15k/8.15k [00:01<00:00, 6.97kB/s]\n",
            "Successfully submitted to Praktikum 2 - RNN - IF4074 2023"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_train[len(X_train) - 1:]\n",
        "Y_pred = models[0].predict(X_test)\n",
        "print(X_test)\n",
        "result = scaler.inverse_transform(Y_pred)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "1kZtiZjpSPwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69aebb8b-6d8c-496a-b3a1-20cb9d38b896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[[0.17667368 0.19873684 0.194927   0.16965217]\n",
            "  [0.29381053 0.28227368 0.2815431  0.29226087]\n",
            "  [0.53995789 0.55351579 0.55811108 0.56234783]\n",
            "  [0.42913684 0.46122105 0.45444907 0.43826087]\n",
            "  [0.77818947 0.90475789 0.88516434 0.78730435]\n",
            "  [0.81111579 0.72025263 0.79128945 0.6993913 ]\n",
            "  [0.51772632 0.52454737 0.51937036 0.51747826]\n",
            "  [0.3968     0.40446316 0.40404535 0.41452174]\n",
            "  [0.30837895 0.29271579 0.29385858 0.3066087 ]\n",
            "  [0.32968421 0.34332632 0.34817715 0.33921739]\n",
            "  [0.18172632 0.1792     0.17225349 0.18808696]\n",
            "  [0.27772632 0.28370526 0.28105375 0.29278261]\n",
            "  [0.28370526 0.28513684 0.27477367 0.29713043]\n",
            "  [0.28513684 0.26728421 0.27697578 0.27069565]\n",
            "  [0.26728421 0.24951579 0.26017454 0.25330435]\n",
            "  [0.24951579 0.27014737 0.25854335 0.25695652]\n",
            "  [0.27014737 0.25574737 0.2568306  0.26269565]\n",
            "  [0.25574737 0.25987368 0.25006117 0.2673913 ]\n",
            "  [0.25987368 0.26922105 0.26107169 0.26756522]\n",
            "  [0.26922105 0.2688     0.26351847 0.2766087 ]\n",
            "  [0.2688     0.25726316 0.25772775 0.27034783]\n",
            "  [0.25726316 0.22551579 0.24467825 0.22008696]\n",
            "  [0.22551579 0.22669474 0.21841612 0.23034783]\n",
            "  [0.22669474 0.28741053 0.28872033 0.2266087 ]\n",
            "  [0.28741053 0.28901053 0.33113123 0.28095652]\n",
            "  [0.28901053 0.31183158 0.32077318 0.29443478]\n",
            "  [0.31183158 0.29448421 0.31971291 0.30965217]\n",
            "  [0.29448421 0.31191579 0.30421662 0.31173913]\n",
            "  [0.31191579 0.29642105 0.30617405 0.30417391]\n",
            "  [0.29642105 0.29978947 0.2954082  0.306     ]\n",
            "  [0.08825263 0.07494737 0.12910856 0.07921739]\n",
            "  [0.19873684 0.16606316 0.19117527 0.16886957]\n",
            "  [0.28227368 0.26812632 0.27681266 0.27608696]\n",
            "  [0.55351579 0.47772632 0.53225675 0.46408696]\n",
            "  [0.46122105 0.54989474 0.59155044 0.47321739]\n",
            "  [0.90475789 0.95477895 0.94062475 0.90043478]\n",
            "  [0.72025263 0.67292632 0.69276568 0.6373913 ]\n",
            "  [0.52454737 0.56547368 0.55452247 0.54721739]\n",
            "  [0.40446316 0.39410526 0.39254547 0.402     ]\n",
            "  [0.29271579 0.27848421 0.28553952 0.29104348]\n",
            "  [0.34332632 0.32741053 0.33308866 0.334     ]\n",
            "  [0.1792     0.17465263 0.17078542 0.18495652]\n",
            "  [0.3232     0.32008421 0.31400375 0.33191304]\n",
            "  [0.32008421 0.30332632 0.30584781 0.31478261]\n",
            "  [0.30332632 0.29178947 0.30307479 0.29773913]\n",
            "  [0.29178947 0.29827368 0.28831254 0.306     ]\n",
            "  [0.29827368 0.30610526 0.29655004 0.30973913]\n",
            "  [0.30610526 0.27848421 0.29181959 0.27652174]\n",
            "  [0.27848421 0.2704     0.26661773 0.28078261]\n",
            "  [0.2704     0.26964211 0.26612837 0.28373913]\n",
            "  [0.26964211 0.23410526 0.25789087 0.228     ]\n",
            "  [0.23410526 0.20926316 0.23016067 0.18826087]\n",
            "  [0.20926316 0.21389474 0.20528505 0.21226087]\n",
            "  [0.21389474 0.17717895 0.20275671 0.18817391]\n",
            "  [0.17717895 0.19098947 0.19598728 0.176     ]]]\n",
            "[[277.07632 242.5237  258.22375 278.00186]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sequences(test_data, seq_length)\n",
        "# Build and train the LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(8, input_shape=(seq_length, 4)))\n",
        "model_lstm.add(Dense(4))  # Output layer has 4 units to predict all features\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_lstm.fit(X_train, y_train, epochs=100, batch_size=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc0zHx4MMlrU",
        "outputId": "53d9efea-af7c-4718-c50a-4fc1dc612030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "325/325 [==============================] - 4s 4ms/step - loss: 0.0620\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0360\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0332\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0302\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0283\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0271\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0247\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0234\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0228\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0217\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0207\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0205\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0199\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0198\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0196\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0193\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0197\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0193\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0192\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0192\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0193\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0184\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0193\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0192\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0190\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0188\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0188\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0187\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0185\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0188\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0189\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0187\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0186\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0183\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0184\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0184\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0182\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0183\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0182\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0182\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0181\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0179\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0177\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0176\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0175\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0174\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0172\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0175\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0174\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0168\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0166\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0166\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0164\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0162\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0161\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0162\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0159\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0155\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0157\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0154\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0153\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0156\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0151\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0148\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0153\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0149\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0152\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0148\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0146\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0148\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0144\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0145\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0143\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0144\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0145\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0140\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0145\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0140\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0142\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0141\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0140\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0141\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0140\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0140\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0138\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0138\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0135\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0137\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0136\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0136\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0139\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0135\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0135\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 0.0135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a68414a5690>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "zw8vNzosOUMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbf1713-13ea-422d-9fb3-76ac81419b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_26 (LSTM)              (None, 8)                 416       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 452 (1.77 KB)\n",
            "Trainable params: 452 (1.77 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis\n",
        "\n",
        "##"
      ],
      "metadata": {
        "id": "6BPOIm0AOb6l"
      }
    }
  ]
}